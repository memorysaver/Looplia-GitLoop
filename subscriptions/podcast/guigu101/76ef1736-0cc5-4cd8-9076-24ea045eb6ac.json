{
  "id": "76ef1736-0cc5-4cd8-9076-24ea045eb6ac",
  "source_type": "podcast",
  "title": "E145 | 对话Meta田渊栋：被Transformer改变的世界与人类AGI的野心",
  "link": "https://sv101.fireside.fm/151",
  "published": "2024-03-28T23:30:00+00:00",
  "author": "硅谷101",
  "summary": "<p>2017年，谷歌一篇划时代的论文《Attention is all you need》掀开这一轮人工智能的开幕式，这篇论文就是大名鼎鼎的Transformer。7年过去了，我们看到在这篇论文的基础上加入算力、算法开启了AI时代的第三次科技浪潮。</p>\n\n<p>今天我们的嘉宾是来自Meta Fair的研究员田渊栋博士，他最近也发表了两片论文都在都与端侧小模型相关，一片论文是《 MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases》中开始卷 10 亿以下参数小模型，主打在移动设备上运行 LLM；另一片论文是《GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection》，由于离应用更近在解决更实际的问题，他的论文被业界很多人问到，而过去五年，他所有的研究都在回答一个问题：神经网络是如何工作的？</p>\n\n<p>今天我们就一起来解读田渊栋最近的两篇论文，也一起聊聊最近大火的Sora、Transformer与AGI。</p>\n\n<hr />\n\n<p><strong>【老罗直播预告】</strong><br />\n北京时间3月31号晚上七点，罗永浩会在直播间卖云产品。之前老罗也带火过很多概念，这次我们来看一看，老罗能不能引领一场企业级IT认知的运动，把云计算这个概念推向大众。他这次的选品涵盖阿里云众多的热门产品，价格也给出了史无前例的优惠，大家感兴趣去淘宝app搜索「罗永浩」，让我们一起围观连续创业者罗永浩卖云产品，以及他如何解决创业者的核心痛点的，直播链接：<a href=\"https://m.tb.cn/h.5BYaoxh\" rel=\"nofollow\">https://m.tb.cn/h.5BYaoxh</a> </p>\n\n<hr />\n\n<p><strong>【主播】</strong><br />\n泓君，硅谷101创始人，播客主理人<br />\n<strong>【嘉宾】</strong><br />\n田渊栋，Meta人工智能研究院（FAIR）研究员及高级经理</p>\n\n<p>田渊栋博士，Meta AI人工智能研究院(FAIR)研究员及高级经理，2018年围棋开源项目（ELF OpenGo）研究及工程负责人和第一作者。曾获2021年国际机器学习大会（ICML）杰出论文奖提名（Outstanding Paper Honorable Mentions)及2013年国际计算机视觉大会（ICCV）马尔奖提名（Marr Prize Honorable Mentions）。研究方向为深度强化学习，表示学习和优化，历任机器学习国际会议ICML，NeurIPS，AAAI, AIStats领域主席。2013-2014年在Google无人驾驶团队任软件工程师。</p>\n\n<p><strong>【你将听到】</strong><br />\n00:05 3月31号晚七点罗永浩直播间卖云<br />\n01:34 正片<br />\n<strong>【“斜杠”AI研究专家】</strong><br />\n02:55 从自动驾驶、围棋开源项目到神经网络研究<br />\n05:52 写科幻小说：不靠谱的想法放进小说，靠谱的想法用来做科研<br />\n07:24 理解神经网络如何工作or 研究大模型，2019年为何拒绝Ilya Sutskever加入OpenAI的邀请<br />\n08:44 最新两篇论文的艰难诞生：曾被两次拒稿，三四年后才看到结果 <br />\n<strong>【GaLore和MobileLLM】</strong><br />\n11:04 GaLore的主要特点：实现在英伟达RTX 4090上进行模型的从头训练<br />\n12:56 算法上改进让4090重获新生，省内存的同时获得高性能<br />\n16:56 MobileLLM：降低神经网络参数仍然保持好的效果<br />\n<strong>【实现AGI的路径】</strong><br />\n17:40 Scaling Law带来的增长会越来越小，我们并没有完全理解为什么Transfomer的效果更好<br />\n19:17 完全无人驾驶难点：人工干预的频率越低，有效训练数据就越少<br />\n23:41 Transformer很难做游戏式的推理：通过理解神经网络的工作原理来改进现有算法<br />\n<strong>【深度理解Transformer】</strong><br />\n24:52 谷歌内部发现算力价格比通信便宜，所以想到要设计一个模型让算力获得更大优势<br />\n26:21 Transfomer vs CNN：没有预设立场，并行效果更好<br />\n26:44 Transformer的缺点：需要大量算力、速度较慢、延迟高<br />\n28:01 强化学习的根本性问题：Exploration（探索）和Exploitation（开采）<br />\n<strong>【Sora、合成数据与Anthropic】</strong><br />\n30:03 Sora的最让人惊艳的地方是所生成的内容一致性非常好，在技术上有根本的创新<br />\n33:07 世界模型并不“高大上”，对未来有看法和预测都可以成为称为“世界模型”<br />\n40:46 用合成数据训练大模型是趋势，其与真实数据之间的边界会越来越模糊<br />\n43:00 合成数据相当于用算力来换数据，但缺乏人类参与的数据会导致学习瓶颈<br />\n44:30 Anthropic的长文理解和分析能力远强过GPT-4，但推理能力难以分高下<br />\n46:30 大模型缺乏竞争的话，也会出现“偷懒”的问题<br />\n48:06 一家独大 or 威慑平衡，Meta开源打破的行业格局<br />\n48:18 大模型侵蚀Google的核心业务，训练和推理很难发论文<br />\n50:20 从玄幻小说到科幻小说：记录人与人之间的关系和思维的火花</p>\n\n<hr />\n\n<p><strong>【相关信息拓展】</strong><br />\nELF OpenGo：由Facebook AI Research团队（FAIR）在2018年开源发布的AI围棋项目。其特点是不使用人类棋谱与累积的围棋知识，仅实做围棋规则，使用单一人工神经网络从自我对弈中学习。<a href=\"https://arxiv.org/abs/1902.04522\" rel=\"nofollow\">《ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero》</a><br />\nMobileLLM论文：<a href=\"https://arxiv.org/abs/2402.14905\" rel=\"nofollow\">《 MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases》</a><br />\nGaLore论文：<a href=\"https://arxiv.org/abs/2403.03507\" rel=\"nofollow\">《GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection》</a><br />\n<a href=\"https://www.zhihu.com/question/404870865/answer/1361685672\" rel=\"nofollow\">田渊栋之前自动驾驶的技术分析帖子</a><br />\n<a href=\"https://yuandong-tian.com/novel.html\" rel=\"nofollow\">玄幻&amp;科幻作品集</a></p>\n\n<hr />\n\n<p><strong>【后期】</strong><br />\nAMEI<br />\n<strong>【BGM】</strong><br />\nInterruption - Craft Case<br />\nStillness Within - Roots and Recognition</p>\n\n<hr />\n\n<p><strong>【在这里找到我们】</strong><br />\n公众号：硅谷101<br />\n收听渠道：苹果｜小宇宙｜喜马拉雅｜蜻蜓FM｜网易云音乐｜QQ音乐｜荔枝播客<br />\n海外用户：Apple Podcast｜Spotify｜TuneIn｜Youtube｜Amazon Music<br />\n联系我们：<a href=\"mailto:podcast@sv101.net\" rel=\"nofollow\">podcast@sv101.net</a></p>",
  "content": "<p>2017年，谷歌一篇划时代的论文《Attention is all you need》掀开这一轮人工智能的开幕式，这篇论文就是大名鼎鼎的Transformer。7年过去了，我们看到在这篇论文的基础上加入算力、算法开启了AI时代的第三次科技浪潮。</p>\n\n<p>今天我们的嘉宾是来自Meta Fair的研究员田渊栋博士，他最近也发表了两片论文都在都与端侧小模型相关，一片论文是《 MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases》中开始卷 10 亿以下参数小模型，主打在移动设备上运行 LLM；另一片论文是《GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection》，由于离应用更近在解决更实际的问题，他的论文被业界很多人问到，而过去五年，他所有的研究都在回答一个问题：神经网络是如何工作的？</p>\n\n<p>今天我们就一起来解读田渊栋最近的两篇论文，也一起聊聊最近大火的Sora、Transformer与AGI。</p>\n\n<hr />\n\n<p><strong>【老罗直播预告】</strong><br />\n北京时间3月31号晚上七点，罗永浩会在直播间卖云产品。之前老罗也带火过很多概念，这次我们来看一看，老罗能不能引领一场企业级IT认知的运动，把云计算这个概念推向大众。他这次的选品涵盖阿里云众多的热门产品，价格也给出了史无前例的优惠，大家感兴趣去淘宝app搜索「罗永浩」，让我们一起围观连续创业者罗永浩卖云产品，以及他如何解决创业者的核心痛点的，直播链接：<a href=\"https://m.tb.cn/h.5BYaoxh\" rel=\"nofollow\">https://m.tb.cn/h.5BYaoxh</a> </p>\n\n<hr />\n\n<p><strong>【主播】</strong><br />\n泓君，硅谷101创始人，播客主理人<br />\n<strong>【嘉宾】</strong><br />\n田渊栋，Meta人工智能研究院（FAIR）研究员及高级经理</p>\n\n<p>田渊栋博士，Meta AI人工智能研究院(FAIR)研究员及高级经理，2018年围棋开源项目（ELF OpenGo）研究及工程负责人和第一作者。曾获2021年国际机器学习大会（ICML）杰出论文奖提名（Outstanding Paper Honorable Mentions)及2013年国际计算机视觉大会（ICCV）马尔奖提名（Marr Prize Honorable Mentions）。研究方向为深度强化学习，表示学习和优化，历任机器学习国际会议ICML，NeurIPS，AAAI, AIStats领域主席。2013-2014年在Google无人驾驶团队任软件工程师。</p>\n\n<p><strong>【你将听到】</strong><br />\n00:05 3月31号晚七点罗永浩直播间卖云<br />\n01:34 正片<br />\n<strong>【“斜杠”AI研究专家】</strong><br />\n02:55 从自动驾驶、围棋开源项目到神经网络研究<br />\n05:52 写科幻小说：不靠谱的想法放进小说，靠谱的想法用来做科研<br />\n07:24 理解神经网络如何工作or 研究大模型，2019年为何拒绝Ilya Sutskever加入OpenAI的邀请<br />\n08:44 最新两篇论文的艰难诞生：曾被两次拒稿，三四年后才看到结果 <br />\n<strong>【GaLore和MobileLLM】</strong><br />\n11:04 GaLore的主要特点：实现在英伟达RTX 4090上进行模型的从头训练<br />\n12:56 算法上改进让4090重获新生，省内存的同时获得高性能<br />\n16:56 MobileLLM：降低神经网络参数仍然保持好的效果<br />\n<strong>【实现AGI的路径】</strong><br />\n17:40 Scaling Law带来的增长会越来越小，我们并没有完全理解为什么Transfomer的效果更好<br />\n19:17 完全无人驾驶难点：人工干预的频率越低，有效训练数据就越少<br />\n23:41 Transformer很难做游戏式的推理：通过理解神经网络的工作原理来改进现有算法<br />\n<strong>【深度理解Transformer】</strong><br />\n24:52 谷歌内部发现算力价格比通信便宜，所以想到要设计一个模型让算力获得更大优势<br />\n26:21 Transfomer vs CNN：没有预设立场，并行效果更好<br />\n26:44 Transformer的缺点：需要大量算力、速度较慢、延迟高<br />\n28:01 强化学习的根本性问题：Exploration（探索）和Exploitation（开采）<br />\n<strong>【Sora、合成数据与Anthropic】</strong><br />\n30:03 Sora的最让人惊艳的地方是所生成的内容一致性非常好，在技术上有根本的创新<br />\n33:07 世界模型并不“高大上”，对未来有看法和预测都可以成为称为“世界模型”<br />\n40:46 用合成数据训练大模型是趋势，其与真实数据之间的边界会越来越模糊<br />\n43:00 合成数据相当于用算力来换数据，但缺乏人类参与的数据会导致学习瓶颈<br />\n44:30 Anthropic的长文理解和分析能力远强过GPT-4，但推理能力难以分高下<br />\n46:30 大模型缺乏竞争的话，也会出现“偷懒”的问题<br />\n48:06 一家独大 or 威慑平衡，Meta开源打破的行业格局<br />\n48:18 大模型侵蚀Google的核心业务，训练和推理很难发论文<br />\n50:20 从玄幻小说到科幻小说：记录人与人之间的关系和思维的火花</p>\n\n<hr />\n\n<p><strong>【相关信息拓展】</strong><br />\nELF OpenGo：由Facebook AI Research团队（FAIR）在2018年开源发布的AI围棋项目。其特点是不使用人类棋谱与累积的围棋知识，仅实做围棋规则，使用单一人工神经网络从自我对弈中学习。<a href=\"https://arxiv.org/abs/1902.04522\" rel=\"nofollow\">《ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero》</a><br />\nMobileLLM论文：<a href=\"https://arxiv.org/abs/2402.14905\" rel=\"nofollow\">《 MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases》</a><br />\nGaLore论文：<a href=\"https://arxiv.org/abs/2403.03507\" rel=\"nofollow\">《GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection》</a><br />\n<a href=\"https://www.zhihu.com/question/404870865/answer/1361685672\" rel=\"nofollow\">田渊栋之前自动驾驶的技术分析帖子</a><br />\n<a href=\"https://yuandong-tian.com/novel.html\" rel=\"nofollow\">玄幻&amp;科幻作品集</a></p>\n\n<hr />\n\n<p><strong>【后期】</strong><br />\nAMEI<br />\n<strong>【BGM】</strong><br />\nInterruption - Craft Case<br />\nStillness Within - Roots and Recognition</p>\n\n<hr />\n\n<p><strong>【在这里找到我们】</strong><br />\n公众号：硅谷101<br />\n收听渠道：苹果｜小宇宙｜喜马拉雅｜蜻蜓FM｜网易云音乐｜QQ音乐｜荔枝播客<br />\n海外用户：Apple Podcast｜Spotify｜TuneIn｜Youtube｜Amazon Music<br />\n联系我们：<a href=\"mailto:podcast@sv101.net\" rel=\"nofollow\">podcast@sv101.net</a></p>",
  "enriched_via": "rss",
  "audio": {
    "url": "https://aphid.fireside.fm/d/1437767933/f0f20376-8faf-4940-b920-84af6c734e2d/76ef1736-0cc5-4cd8-9076-24ea045eb6ac.mp3",
    "type": "audio/mpeg",
    "length": "78033243"
  },
  "duration": "54:10",
  "season_number": "4",
  "image": "https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/f/f0f20376-8faf-4940-b920-84af6c734e2d/cover.jpg?v=6",
  "season": "4",
  "tags": [
    "田渊栋",
    "MetaFAIR",
    "Transformer",
    "神经网络",
    "Sora",
    "合成数据"
  ],
  "archived_at": "2025-11-28T15:01:26.742202+00:00"
}