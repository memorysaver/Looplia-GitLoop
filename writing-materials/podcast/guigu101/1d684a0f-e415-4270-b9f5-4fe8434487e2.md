---
id: 1d684a0f-e415-4270-b9f5-4fe8434487e2
source_type: podcast
source_key: guigu101
title: "E211｜站在内容创作者与机器人的交界处：聊聊3D数字人的进化"
url: https://sv101.fireside.fm/222
published: 2025-10-24T00:00:00+00:00
downloaded_at: 2025-11-28T14:15:16.129110+00:00
---

# E211｜站在内容创作者与机器人的交界处：聊聊3D数字人的进化

欢迎收听《硅谷101》我是红军9月的最后一天Sora 2发布它可以把一句话变成一段十秒的短视频而好莱坞呢完全由AI创作的演员Tili Norwood他诞生六个多月就获得了6.5万的粉丝他发自拍代言品牌却从来没有真实存在过所以我们看到一个趋势数字人他正在成为新的内容生产者但是我们说在屏幕上生成一段2D的视频到与一个3D的数字人实现一个比较稳定的实时的互动这中间其实还是有很多的技术壁垒的本期嘉宾柴金翔教授他是2000年就已经进入了卡雷基梅隆大学去研究机器人但是因为机器人它的应用与落地都非常的困难所以呢他们团队反而成为了世界上最早用AI做3D动画的团队在这18年的时间里他几乎都在做同样的研究从机器人到好莱坞的AI动画到今天啊其实我们又在讨论一个非常前沿的问题就是3D数字人的模型是不是又可以反过来去驱动机器人所以这听起来其实有一点点像一种轮回但也是一种新的开始那这一集我们就来讨论一下这一轮的人工智能浪潮到底是如何去改变3D数字人这个行业与机器人领域的而在好莱坞与游戏产业里到底谁又是这项技术的受益者那今天跟我在一起的嘉宾是魔法科技的创始人与CEO柴金翔教授Hello柴教授您好Hello大家好我以前是在美国当教授是18年的时候回去创立了魔法科技做了大概有七年多的时间很高兴今天有机会和大家分享一下关于3D数字人和聚生智能的一些观点因为正好是在采访您前几天就是硅谷大家都在关注的一个大事就是Soral2放出来了然后它做成了一个类似于社交媒体的形式就是我最近是被Sam Altman刷屏了好像每个人都会拿它去做一段demo包括我们前几天刚刚开了硅谷101的科技大会我们就生成了一段让Sam Altman帮我们去宣传我们大会的Solo2的视频看起来它在屏幕里面的形象就是一个比较数字人的形象这个对你们的业务会有影响吗我自己大概也去快速地体验一下我觉得Solo2相比Solo1的话进步是蛮大的无论从画质也好包括新的视频的那种形态我觉得这里面一个核心的点其实还是一个文身视频的能力了它主要的形态是以人为中心的以前可能Solar One的时候还是有风景啊各种其他的一些视频的内容的形态它可以让视频的人做各种各样的事情跳舞也好坐在那儿给你交流也好吃东西也好等等等等我也大概用了一下第一感觉的话其实视频生成的现在才是十秒钟的时间还是没有跳出对于实践的限制文身视频那第二个点其实也特别特别重要大家一直在说物理上的一致性基本上大家看刷屏的时候很多效果其实还是蛮好的但你真正自己去做的时候还是有很多瑕疵太多了对他那个视频里面有皮卡丘跟一个唐老鸭在总统竞选的一段辩论他可以在原视频上改我就说把这个辩论变成一个在硅谷101上关于AGI的辩论主题是Alignment 2025是我们活动的主题但你仔细去看的话他后面的Alignment他那个字就是错的就开始乱买了是还有一个点特别重要对视频这样的内容创作者来说你不仅想生成视频它的背景里面如果有字错了或者人物的有一知心的问题那你最想做的说我能不能edit一下能把它变好其实他现在这个能力也还没有那更重要的是他现在是容许你用plump去生成这个视频你没有能力去精细化的去控制这个人的动作啊表情啊等等等等有一个点它是特别的好它第一次让人看到了如果有一个大模型其实你可以让人做各种各样的动作在视频这个层面上那我们是在做3D素人这方面的研究包括产品我们认为假如说最后我们像做人人可以在交流跳舞可以在娱乐大家那最后的大模型会是什么形态它的训练数据会是什么形态那你看到Solo2他就说我用所有的视频作为输入作为训练数据我就可以做这个最近Jenny 3出来了以后其实是它其实是3D的生成了给你一种标护型的感觉是吧但它不是人它是关于这个场景相关的到最后如果咱们做大模型如果做人的话我们会觉得最后可能需要2D加3D的所有的训练技术在一起我们是希望你生成这个人的时候不仅是没有这个10秒钟的限制你还希望它没有瑕疵物理上是准确的你能控制它最后它是实时的而且成本很低因为我们自己现在做3D数字人的时候除了我们3D的训练数据其实我们也已经可以使用大量的一些视频的数据一起结合作为训练数据来做这个大模型的整个3D数字人的表达力的模型你看视频这个数据如果你的模型做了好其实还是有一定的生成能力的以前我看Sora 1的时候我觉得好像不行因为你觉得Sora 1都是场景式的生成然后Sora 2里面它基本上因为你要让视频有生命力就必须有一个主体这个主体要么是人要么是动物然后它的生成看起来也比较立体所以其实我简单总结一下Sora2跟你们的整个的3D数字人的生成最大的区别是它是文 生 视频你们相当于是文 生 3D然后这个3D它可以是在VR领域里面进行展示的比如说我戴着一个VR头盔我可以360度地去看到这个人同时这个3D未来可不可以用于机器人这个我们待会也可以去讨论一下核心是这个区别是就是2D跟3D之间的区别Solar2呢还是文生2D的视频那如果3D的话你放在VR里面它就跟我们现实生活中是一样的那3D还有个好处它能让你控制就像人一样你让它怎么动就怎么动但你2D呢因为在这个我们叫像素这个层面其实要对它进行精准的人的动作表情的控制相对来说会比较难因为我看见你们公司的数字人其实也会用于这种屏幕的展示有一点我可能很难区分的是我知道很多公司的那个展示屏的数字人都是你们来做的假设我进到一个展厅看到一个屏幕上的非常立体的数字人他有动作声音表情跟我看到Sam Altman在一个视频里的数字人就我们说除了时长的这个区别他在核心的技术上他的区别是什么呢一个区别呢在屏幕上这个数字人其实它担当的角色是人跟机器之间交流的一个载体那人跟机器交流的时候其实你是实施的互动的我们是希望一般来说端对端小语两秒钟或1.5秒的延时才可以做那不能说像你生成一个视频要等个10分钟5分钟这是不可能的如果这往下走呢屏幕里这个数字人如果是在展厅里面你希望所有的介绍回答问题它的动作不能出错如果有一点瑕疵那你肯定觉得哇这个不行啊所以这里面跟骚扰兔有一个很大的区别你不能有瑕疵你的物理上是准确的不能有瑕疵物理上准确指的是人他的表情动作他有一致性他不会突然出现说我的胳膊突然衔接不上了不能是这样子的不能是说大家以前可能看了比较多当你用纹身饰品去做的时候手指是个特别难的事情经常手指会多出了一叠或者从最真的手OK可能到五秒钟的时候忽然多了一个手指或多数类的一截或穿插也好那你在实际的交互中特别是一个人的时候他是一个服务人员或者他是个销售人员跟你讲解产品的时候你肯定是不希望这个体验很差的他一定要准确那最后一个点呢当你把这样的3D的数字人部署在一个终端上的时候你的成本不应该高因为终端本来今天来说他本身这个屏幕可能也就一万人民币那如果你说生成这个视频给他交互了20分钟即使他能实施做他说一年放在那我也得花很多什么钱那就没法做了但今天从Solar的角度或Solar图的文生视频的角度来说其实这个成本它是不能scale up的它的成本是多高用你们的这个成本是多高为什么成本之间会有这样的一个差距我不一定能给一个具体的数字但我可以告诉你一个量级我们现在比云合成的成本比如说云合成现在大模型做的我比它的成本可能是几十分之一吧这里面就很核心的一个点就是2D跟3D之间的区别其实3D对人类说其实我要去描述一个人他的动作表情其实他只要有几百个参数就行了就是人的肌肉可能就是大几百块那你只要去控制这些肌肉就可以了那你下一步就是用3D的渲染把他3D的内容变成视频然后要3D的解算因为不仅有人的表情动作还有头发衣服我们需要用物理解算这两个事情如果你用AI来做其实我们最大的一个核心是说你内容生成以后你要用游戏的引擎非常昂贵的GPU来做当我们把这两个事情用AI渲染跟解算做了这就使得咱们的成本主要就是生成每一帧生成这几百个参数的成本跟大模型生成Token其实是一样的所以它的成本就非常非常低如果说你是从纹生视频因为你是没有结构化信息你全是像素所以在这个事情上使得你整个的推理整个的生成的过程你的成本会非常非常非常高OK所以我理解现在你们之所以能把成本降下来是因为你们有一个自己的端模型可以这样理解吗是我们有一个怎么把文本变成我们所谓的3D多模态表达能力的一个模型其实就是从文本声称语音表情的参数动作的参数字体比如手势的参数我们把这些参数传到终端终端就是可以是Ped或者是终端一个大屏上我们有AI渲染AI解算把它变成视频这个渲染呢其实因为我们用AI做的所以它对于终端的算力的要求是极低的我们现在都可以用一个比如说国内这种几百块钱的芯片比如说RK3566瑞辛威的我们在端上就可以跑了比如说它要跟人做实时的互动跟问答这种还是在端模型上还是说你后面除了你自己的这个端模型在表达的内容上你会去接大模型这是个很好的问题了那我在跟你交流的过程中首先我的眼睛看到你我的耳朵听到你这个我们叫做感知嘛那大模型其实是有理解跟决策能力的然后我们做那个事情是把文本直接变成了语音跟我撕探身体的动作啊表情啊手势啊就相当于一个人他不仅仅只是有语音也不仅仅只是有表情或者表情比较木囊他是动作以言手势都是一体的就是人与人之间的交互嘛接着你去GP的话你可以输入声音图片它最后输出文字我们做的是说是文字到3D的多模态这个输出比如说咱们俩要交流就人跟一个数字人他其实需要两个模型一个是就是像缺GP这样的多模态到文本的模型第二个是从文本这到多模态的3D多模态的模型就语音的输出动作表情手势的输出我们只有自己的垂育的大模型但我们也可以用比如说现在国内像谦文的也好DeepSick也好或者多包的模型也好跟我们接在一起形成端对端的人与素人之间的这种像人一样的交流的一种体验所以你们从多模态到文本是可以用大模型来做用大模型来做然后从文本到多模态就是你们自己的这个端模型自己的我们叫做文森3D多模态大模型这已经是一个产品可以发布新云平台是这个产品我们会在10月份发布现在我们是在测试Testing Mode然后我们现在可能有几百个的B端的企业客户在测试那也有付费了已经那我们预计时间会在两周后吧发布我们的纹身3D多摩碳这个模型因为我们自己在做的过程做了很长很长时间从我回到20多年前我读研究生开始到今天我们花了很多的精力做我们希望大家不要重复造轮子能够把这个能力提供给所有的开发者能够把我们的能力积升到他们的应用中去哦 了解我自己觉得很有意思的一点就是因为我知道你们公司之前其实是在做3D数字人的所以我理解随着就是你们要发的这个星云平台所以你们其实就是从一家3D数字人的公司到了一家3D数字人的平台公司我这样的理解是对的吗差不多我觉得差不多对 是的之前我在NVIDIA的发布会上曾孙黄他会非常自豪地说你看到的我不是真的我然后他是一个自己的虚拟的3D数字人跟大家介绍比如说他后面有一个壁炉然后他在前面介绍渲染的非常非常的真实他经常会用他自己的虚拟人去讲他们的渲染能力有多强大他们的显卡有多强大他那个的成本大概有多少这个成本其实蛮高的但你问的问题特别特别好因为他今天做的其实还是视频的输出通常来说造这个人如果像老黄造成这样子的话通常是需要研发团队配合美术团队一个team来做这个事情按照业界的它的成本基本上在美国的话造国内的最顶尖的美术团队来做基本上在10万美金左右10万美金一个人对 差不多做到就是他们发布会的那个效果然后再包含视频它只包含把这个人造出来视频呢如果要做呢可能成本也会按秒算这个其实在我们通常会讲3D数字人其实还是在专业级的内容生产就还没有到每一个人都能生产这样的3D数字人能生成这样的视频对 我记得其实每次去那个游戏的展会会感受非常明显大家怎么去造那个3D数字人以前我们是让一个演员他会有很多的动作捕捉的团队同时会有一个环形的摄像机把你身体的每一个部位都拍再来建模再一步一步地把它还原出来这是不是也经常是好莱坞使用或者游戏公司使用到的一种方式对刚才讲到了专业级的造人无论是三级游戏公司包括好莱坞的Evatar或者老黄这样的数字人其实整体来说它是两%的东西那第一%我要把这个人造出来像你嘛一般我们就要扫描你会有很多相机在那那你坐在那你做个种表情把你的人的整个的几何几何指的意思就像雕塑雕你的人的这个形象一样然后这把人表面我们叫纹理啊包括这个材质啊把它给重建出来然后生成这个人包括人本身的肌肉我们用学术化的语言来说叫建模跟绑定第二步你要让它动起来嘛你看到的就是身上穿一件洞骨的衣服上面有各种点旁边有很大的一圈巷枝能把你的动作捕捉下来这去驱动刚才那个造完那个人这也用渲染引擎或者用离线的渲染的引擎输出视频其实它整个过程从建模绑定造这个人到动画让这个人动起来这一道后面输出这个人的视频那整个过程都是非常非常贵的这一套我理解是在大模型之前的时代好莱坞跟游戏公司经常会用到的一种方式那现在有了模型这一套在这两个行业里面仍然是主流吗还是说他们其实也在探索我能不能用3D直接去生成人其实这个问题特别特别好这就说到这个行业的一个特性其实3D内容本身的AI化它取决于两个事情一个叫高质量的数据一个是AI的算法是不是能对3D的内容做大模型咱们今天看到了所有的影视动画公司游戏公司他们比较擅长的是做内容把美术把3D的模型做得很好很逼真但绝大部分这些公司应该说AI的能力基本是缺乏的因为它走的是跟我们现在互联网公司跟科技公司两条线它两条线之间的交叉是很少很少的其实他们当然也希望拥抱AI但是呢今天他们在这方面的能力是欠缺了AI公司当然算法能力很强但它其实是没有数据的但是如果说是3D的内容首先得解决你得有大量的3D的高质量的内容你才能做大模型所以这是他们的目标但其实做起来现在对他们来说就是两个行业没有交叉就是大模型公司缺这些好莱坞制作公司的数据然后这些好莱坞制作公司缺AI的算法我可以这样理解吗是 基本上是这样但我看也有一些公司开始在尝试做了你们其实我理解在这一轮AI浪潮之前你们做这个数字人跟积累这样的数据时间也很久了是的我们是18年成立的当时最主要做的是我们为B端的公司比如说游戏公司或者影视动画公司或者企业像做3D的虚拟偶像的为他们去提供3D的内容制作的那时候其实做用AI加美术一起来提升效率提升质量然后在这个过程中当然AI的能力也在提升我觉得大家都要突破了一个点就是3D内容的高质量数据你没有数据你AI算法这怎么厉害都没法干对 从18年到现在是2025年你们大概积累了多少数据可以透露吗就像我们动画数据我们前面是为企业服务了后面呢我们就自己来做动画数据我们现在如果将3D高质量的动画数据我们在1000多个小时左右其实这个数据可能跟视频的数据来讲或者文本的数据来讲是小的但是如果你考虑到它的成本一条数据大概要多少的成本大概比如说像我们现在要的比如高质量的人脸的动画手势 表情这些我们叫动画数据一秒钟至少在1000人民币左右在国内这个一方面当然你说成本其实还有一个点你得找到团队有非常强的能力把质量做得这么高所以整体来说它的成本它本身的数据的量其实是很难很难在短时间内积累起来的很有意思所以数据是你能训练成这样的一个模型的一个核心要素我觉得数据是最最最最核心要素如果你没有数据其他任何的研发都没法做因为我们现在有的是3D的数据刚才其实我们也讲我们其实还有的是有视频的数据视频的数据是哪里来的举个例子你现在在网上看到了有人在走路也好有人在跟人交流也好这个就是纯粹的视频的数据它是没有3D信息的但是另外的3D的数据呢其实我们现在可以是把两者融合起来去做模型的训练OK 有意思你为什么当时会选3D数字人这个领域我是2000年去卡梅读博士当时在机器人研究所我就做的是这个方向当时我的博士论文就做的是怎么能够创建一个可交互的3D数字人怎么用AI去做动画我们那个团队应该是世界上最招用AI做动画的因为那时候也刚凑巧运动捕捉刚刚在2000年的时候那你有了动画数据你就可以去做AI了所以从那个时间点开始我开始做3D动画 3D数字人06年毕业去TaxM当教授其实也一直在做这个方向那时候做动画主要在我们叫图星学图星学专门是为影视动画公司游戏公司这个行业服务的所以那时候我们发了很多论文全是关于3D数字人跟3D动画相关的创业呢其实18年也是做了同样的事情所以这个事情呢应该说坚持了有20几年吧当然在学校里做了还是以研究为主了然后我知道您的PHD的导师是Jessica Hawkins她其实主要是研究人形机器人还有3D的数字动画的而且她的博士生导师是Mac Rebuilt是Boston Dynamic的创始人现在最有名的机器人公司也是特别早的一家机器人公司所以看起来整个的3D生成它最开始的应用就是在好莱坞领域的我导师Jessica Hawkins她是CMU毕业的也是卡梅89年博士毕业Jessica以前是在博士的时候他是做机器人的那时候机器人也叫人形机器人但只有一个脚人形机器人只有一个脚为什么只有一个脚因为两个脚那balance太难了其实你也看到过现在人形机器其实在几年前两个脚的人形机器还会跌倒balance是一个非常重要的问题他那时候做的是用物理运动控制的方式动力学的方式控制机器人走跑跳单腿的是吧他毕业了以后呢很奇怪他进到的方向是到了图星学或动画这两个方向大家可能觉得好像没有联系其实他当时想法是说我在实际世界中能让机器人动起来那我是不是用同样的方法能让虚拟世界中的3D的数字人能让他动画能够动起来所以他是全世界第一个用物理用动控制的方法来做数字人的动画的他到了他是叫Georgia Tech做Professor他就说我们叫做Physics-based simulation跟control做动画然后他是2000年的时候又回到了卡梅当教授但2000年呢动画数据呢慢慢有了刚才讲到运动捕捉的出现然后他又开始我就是他在卡梅拜的最早的博士我们是那时候是最早用AI来做动画因为你有了数据了然后他又做动画做完了以后呢后来就觉得这个动画挺好的用AI去做那么反过来是不是还能去做robotics这个行业现在大家可能知道了很多做robotics做很厉害的人其实以前都是做动画的比如说Sagay Levine他是Pi的联合创始人也是Berke的教授但你肯定都不知道他是在Stanford拿了博士学位他在Stanford读博士的时候就是做动画的他是用物理的方式用运动控制动力学的方式来做动画他毕业了以后说我这个人做动画我也能做机器人他后来当教授的时候就是可以做机器人难怪派他们的核心思想是要解决整个机器人的大脑的问题就是软件层的问题他就是希望通过模型层来指挥机器人我觉得这个跟他最开始不是从硬件研究开始的它是用机器人去做动画听起来是一脉相似的是的的确是的我再给你举一个例子我还有一个很好的朋友叫Karen Liu她现在在Stanford当Professor她是同时做Animation做Robotics她以前是在Georgia Tech当Professor那也是做Animation做Robotics那我们还有其他那个时候做Animation的人后面都做Robotics或Robotics and Animation就这两个领域是非常非常相通的因为都是3D一个在虚拟世界一个在物理世界你都是要驱动人都是让机器人像现实间的人一样能够去驱动他为什么那时候很多人做动画因为动画这个事情相对来说会比机器人会简单一些因为机器人你是有个本体的你打个hardware就老半天动画在三维的世界当中你至少不需要打这个hardware第二点现实间呢其实是有很多的限制比如说重力啊或者这个房间的限制啊或者这个机器人硬件的限制啊动画这个事情是没有限制所以那时候其实就有很多做物理的人开始做动画做动画这方面也分成几派一派呢几物理来做那比较有名的Jessica肯定也是了包括在UBC的Michael Van der Pan她是我博士的Committee的成员她一直做Controller做运动控制的那时候做动画的中心其实也在卡梅Karen Liu她的导师叫Zoran Popovich她其实是从卡梅毕业的那个时候做物理的整个的动画这一波人其实人很少很少那时候国内基本上没有人做动画可能欧洲也没人做其实在美国可能最主要就是那么两三个组了后面呢动画有一个大的飞跃是从2000年开始的那时候最主要的原因是做动画的时候有数据了运动捕捉把数据有了以后那这个事情实际是使得这慢慢慢慢大学可以用AI做那时候比较早现在叫强化学习我记得最早的做动画的paper应该是04还是05年就用强化学习去做动画其实虚拟世界跟实际世界是差不多的它唯一的区别就是实际世界中你有硬件的限制但底层的方法其实很类似很类似我们在讲小脑做的事情动作的规划运动的控制这个流派到现在动画的人也有人在做Robotics也有人在做如果到现在最新东西出来我们叫VLOA视觉语言动作模型这是一个新的这个讲的是大脑但小脑这个事情其实是动画跟机器人是蛮类似的很有意思我们之前聊很多好莱坞的节目的时候就有听众问我说硅谷101不应该是一档技术节目吗然后你们在讨论AI的时候不是应该多聊聊技术吗我就说其实好莱坞是还蛮重要的驱动整个的科技向前发展的一层而且很多AI技术它最开始用到的就是在电影制作上是你们有没有想过就比如说把你们的整个的3D数字人的产品用到更多的好莱坞造人比如说你们公司只是用深层的这种方式因为你们已经训练了自己的端模型你是可以输出一个数字人的模型的就是其实底层的技术你就有了嘛就可以去把一个不太动的演员让他活动起来我觉得这可能是对整个好莱坞的一次降维打击就是我们刚刚提到了很多他们怎么去用AI技术跟机器人的技术去互相的促进跟发展的那现在听起来那一套技术已经是一个有一点点落实的技术虽然说我们现在生成效果还没有那么好但是我觉得现在整个进展很惊艳了是的 其实你刚才讲到一个非常非常重要的点当我们讲一下技术的时候其实这里面有几个关键的点一个是它的质量好雷乌质量可能最高的这往下是三级游戏这往下可能是我们在生活中有一些如果说交互做的比较简单的第二个就是我们在讲的成本那第三个事情其实又讲到了它的应用场景在哪里如果你要做好莱坞这个方向它Fidelity它的质量可能是特别特别重要因为它可以等100个小时或200个小时或花更多的钱去等你的高质量但是在时事交互里面它可能是说我今天我等不了那么多时间我就要马上能看到这个结果能够给它交互我在质量上可能不一定要像好莱坞那么高的质量但是可以做好莱坞的IP的衍生对 衍生品肯定可以当然如果好莱坞要做这方面也可以做你需要更高质量的3D的数据来做这个AI的大模型这一块东西呢在我们自己在行进路径上我们可能更先后的顺序对于我们自己来说可能先是到日常生活中比如说交互 服务 陪伴这道游戏 这道好莱坞因为难度来说其实好莱坞如果要做到那个那你的难度是很高很高因为你的质量也很高但能生产这个高质量数据的人全世界可能就没几个正好是在我们采访前几天我看好莱坞他们已经造了一个叫Tilly Norwood的女演员这个女演员呢她是完全有AI生成的但是如果你去follow她的因子她其实看起来跟真人就非常的像她每天也会自己喝咖啡也有自拍照然后也有自己的生活如果你只看她的社交媒体你是很难区别她是一个真实的演员还是一个数字造出来的演员的像这种极数以你们的平台比如说开发者再接一个你们的API可以做到吗还是说他需要更多的其他的方向的辅助他的现在的做法其实还是我们PGC的制作的方式他还是二比生他是文生视频的方式对但你们如果来做降维打击啊对但是这里面有一个点是在园语族比较火的过程中其实美国有一个虚拟Virtual Idol叫Leo McQuayla他在Inns上可能有个一两百万的粉丝他可能比今天你刚才讲到的Tilly老五的粉丝要多很多很多刚开始大家都不知道它是个虚拟的偶像所以我们今天在讲这个方式呢其实在讲更多社媒运营怎么能生产这种专业的内容去打造一个人设那这里面有一个特别要注意的一个点咱们今天在文生视频的过程中因为你不能保证每次生成的视频是百分之百准确的但它可以有个时间因为它如果要每天post一张picture的话或一段视频的话它每天可以我生成个一百段从中挑一个就可以了如果是在这样的情况下如果你要去做real time是不可以的那我们反过来再来看我们现在这套技术如果post到比如说比如说他要开一个现场演唱会当然是可以做的纹身视频有一个好处因为它是基于视频类训练的你觉得这个真实感好真啊虽然可能有一些物理上有的时候会不准确那我们现在采取了3D的方式做的时候其实我们可能不是像做的跟现实界的那个真实感一模一样的我们其实还是在这个交互性上嗯 了解了解就是它更有一个实时性如果是要实时性它在训练上它要最侧重的是什么呢实时性里面我觉得它的核心的点我们一直在讲这个latency这个时间比如说文本书的延迟是特别特别重要如果你的时间要take比如说十分钟一分钟或三十秒或一秒钟我们基本上我们现在做到是五百毫秒到六百毫秒之间如果说你是文生视频的它就没有这个需求等个五分钟可以等个十分钟可以还有一个你部署的时候也很重要因为你是个实时交互的一个用户在跟他沟通交流的时候很有可能我同时有100个用户或1000个用户1万个用户给这个3D的AI数字人在做交互那如果说每一个人生成的内容都会不一样给交互那如果你的成本很高如果同时有1000个人你要成个1000倍这个成本就是1000个用户跟同一个数字人交互那它可能就是一个高并发的场景所以你在后面就是要去做额外的服务器的部署是如果说你今天成本又很高延时又很长那根本是没有可能做了对问一个稍稍有一点敏感的问题你可以选择答不答案那你们现在整个的API接口放出去我相信它肯定有一个基础的接入成本你觉得它是能赚钱的吗是肯定的因为我们在真正的发布这个平台之前因为我们已经有逼端客户了你国内做AI公司你得商业上这个仗得算得过来除非你是字节阿里腾讯是吧所以在这里面就很核心的一个点也是我们在过去的半年里面一个最大的突破吧从我们的交互能力也好API也好半年前其实我们已经做好了但是我们那时候成本很高就是刚才讲到服务一个人的成本当时要一张显卡基本上是两三万所以那时有很多很多的B端的客户进来说你这个东西能不能让我用一下然后一问我们这个价格是这个人家不用了所以这个成本是怎么降下来的因为我们是3D的内容那3D内容有一个特别特别重要的所有的影视动画公司游戏公司逃出去一定得有渲染引擎跟解酸的引擎这个我太懂了因为我们那个做视频那个渲染真的是太耗时间了对对对如果你3D的内容如果要支持实时每一路一张显卡就是为了做3D的渲染跟解算那么我们用了可能最好的引擎叫Unreal但成本放在那那我们当时一直在想如果说我没有把这个成本这张显卡给干掉我们再谈应用真正的让大家都用比如说在刚才讲的展厅里的大屏也好手机上也好Pad上或电视机上根本不可能其实我以前是觉得解决不了的但技术有的时候就是很奇怪忽然想到了一个方法那我们非常幸运吧把渲染跟解算用AI做好了不需要渲染引擎不需要渲染引擎所需要的这个显卡我们看在非常非常便宜的端上可能一两百块两三百块钱的这个芯片上我就可以做渲染跟解算所以你用AI的方式端到端的模型解决了渲染的问题渲染问题只是其中的一个前面还有一个问题是说从文本生成3D的动画表情所需要的参数跟语音其实你要通过渲染跟解算的方式才能做的那以前需要显卡其实我们是两趴了我们就要分成这个模型了第一趴是解决的是文本到语音到3D的表情动作自探第二趴通过3D的动作表情自探的参数输入让输出了它对应的实时的视频这样使得我们就成本就比语音的生成的成本还低那你觉得如果你能做到这件事情把整个渲染的成本大幅降低这次的整个的生成是AI技术对Unreal这些游戏引擎公司会是一次冲击吗对NVIDIA它可能就是一个左手跟右手的关系它那一部分失去的卡我认为对游戏公司来说更多的是个机会不一定对Unreal是个特别好的事但对游戏公司因为每个游戏公司今天特别你上一季游戏你去run的时候你一定是得云单有显卡或者在手机上虽然你比较强不然手机也经常很热你玩的时候所以这个事情对于游戏公司来说可能是一个好事对于渲染跟解算引擎所需要的将来是不是用AI的方式就可以把这个事情给解决掉不需要引擎你不需要填卡就可以玩游戏了那游戏可能是到时候也可是Aeroware或者将来真的用原宇宙的时候那大概在这个虚拟世界中的时候它的成本就会很低很低也许那你觉得现在用AI的方式去解决渲染的问题它的解决质量跟原有的游戏公司的渲染的质量大概到了一个什么样的进度位对于我们这个特定的应用场景基本上是一样的因为我们在做这个事情你的输入的训练数据就是用最高质量的游戏引擎渲染的然后你只是有大量的数据同时去逼近跟原先用游戏引擎的效果而已包括我们自己做了side by side comparison就是左边是用游戏引擎右边是用AI没有一个人能看出来左边跟右边的区别哦那这个非常的颠覆啊这个对这个对于我们类似是个非常非常特别是我们今天说我们希望把3D数人放到每一个终端每一个屏幕它就是一个最重要的事情也许我们从技术上我们说纹身3D的多模碳的这个大模型我们能做但是你真的要部署下去的时候low cost的这个事情它就是一个最最最最重要的问题那对于游戏公司自己呢他们也有很多的游戏人物他们也需要大量的渲染然后他们也需要很多的卡对它可以预渲染其实还是需要卡或者要游戏引擎就像我们生成训练数据一样或者大家今天看到比如说做巨星智能机器人你要采集很多数据但是采集完了训练完了以后你就不需要这个数据就在实时比如说在机器人去抓东西的时候你就只是寄模型去做而已你就不需要这个数据了所以他们还是模式不一样他们对实施性的要求没有那么高就是游戏公司的AI渲染的问题游戏公司的AI渲染我觉得将来也一定会走向到最后实时玩游戏的时候不一定要真正的游戏引擎我认为最后可能是用大模型这种端的能够针对某个特定游戏某个特定应用场景的这样的一个AI的渲染引擎就可以做了也许这个渲染引擎会比咱们今天用游戏引擎跟用解算渲染的方法可能更便宜为什么这样我听下来觉得Unreal跟Epic Games如果不赶紧更新是有点危险的但前面训练数据还是要从他们那里获得但实时的时候就不一定需要他们其实你看到Genie 3是吧它有很多训练数据就是用游戏引擎生产出来的呀当你它生成视频的时候它现在说我有3D的这种感觉你可以跟它交互那这时候它其实不需要游戏引擎了你觉得现在如果接入你们的这一部分开发者它可以现在用这一套3D数字人的平台去做一些什么样的事情它的场景有哪些呢我觉得这是一个非常非常好的问题我们真正在做这个平台的过程中其实我们已经有很多客户合作过或者是找过我们那么我们自己会看到了一个最最最最重要的应用因为现在大家可能在国内大家慢慢已经有大模型了将来有一天可能大模型会出现在各种中端在你的手机TabletPC或者在你的线下大屏或者我们现在有很多小的全息屏放在桌子上的陪伴的假如说大模型再出现这次中端了你怎么跟它交互你交互难道今天还是在文本框里打字吗或者用语音给他说话对他空气讲话一样我现在觉得打字的效率太低了就是很多时候尤其是在一些突发跟实时的场景下可能是我越来越依赖模型了来不及是的 是这样子有一个非常有名的心理学家70年的时候有一个发现说人与人之间的沟通交流可能5-6是视觉信号语音信号呢占30%语言这个信号呢可能这样7%才在交流的过程中所以我们一个最大的应用场景今天在护许的就是我们想把我们的3D数字人这个能跟用户交流的通过语音 动作 表情 姿态就是让它到每一个屏幕上去从非常大的这种显示屏或展厅的屏幕到电视机的屏幕上再到电脑 手机 车机到最后的迷你全息屏当你给它交流的过程中其实你就像跟人交流一样有一个有意思的点是大家现在看到大模型它的交互从文本到文本的输出文本到图片的输出我觉得这个全部是在XGPT发布的时候我们可以说OpenAI定义了它的交互方式但是其实我们看到现在整个OpenAI也在升级这一套的交互方式所以你觉得未来的交互方式它可能就是一个数字人对数字人的这样的一个交互就是像人跟人之间的交互一样而不应该是一个文本对语音或者文本对视频的这样的一个交互是我觉得将来咱们人跟机器或人跟屏幕的交互一定是人跟人之间的交互一样那今天QRGBT刚才讲到了他们其实做了多模态到文本的也做了文本到视频文本到图片就差把这两个串起来了但它没有文本到多模态表达如果加上了多模态到文本那两边串起来就是完全就像我们现实生活中一样我看到你的表情听到你的说话我自己知道要说什么然后同时我有表情动作声音那每个人都要渲染一个自己的模型吗那我觉得将来每个人肯定会有一个分身至少我觉得企业来说它肯定会有统一的一个形象比如说我是做Customer Service的所有的用户看到企业的都知道这个是这个企业的Customer Service或者它是一个企业的虚拟人的一个形象对对对个人肯定也会有个人我们叫个人的分身那大家跟你沟通交流比如说红军你可能对于某方面非常专业那你是个专家你将来肯定会有一个你自己的分身你休息的时候你同样可以跟别人去沟通交流提供你的反馈等等等等如果我们把你们现在的模型放在一起综合去看这个能力的话你觉得它最强的一点是什么就比如说我们自己现在在看到很多的2D的视频渲染的时候我觉得最大的一个痛点在前几年可能是这个口型对不上它有一种虚假感或者眼神它很空洞那你觉得现在你们的这个3D数字人在应用到不同行业的时候大家最大的痛点是什么你们是怎么解决的我觉得这个问题非常好我们自己在跟客户沟通交流的时候我们受到的feedback永远是几个问题第一个问题就是你提到的质量好吧一个是它的语音动作表情它的唇形是不是自然是不是像真人一样然后另外一个事情呢就是延时我跟他聊的时候是不是我说一句话他等五秒钟才回来我肯定没有耐心了第三个事情其实他们非常关心它的成本如果非常贵基本上从客户的角度因为他是要考虑ROI的体验是提升了但是如果付出的成本很高对他来说他也不一定要去做所以我们从整个的核心的点来说这三个问题是我们正在落地我们要规模化的过程中我们叫三座大三如果还有一个点可能是说我们要想让巨生智能素质人能够到多终端无论是说大屏上小屏上手机APP知识并发这里面可能牵涉到不同的操作系统不同的芯片的算力那我们解决这个问题的方式质量跟延时最主要用的是我们的大模型提升它的能力质量这个事情当然训练数据是最重要如果你的动画的训练数据这个人3D人的质量很差你就根本做不好然后另外一部分就是大模型本身的能力你能不能通过文本去声称语音表情动作包括纯形能不能让它匹配同时我还能从文本里面提取一些情绪比如说它笑或者打个招呼它能够自动生成这些关键的意图包括你的TTS语音生成是不是也是有情绪的那这个事情其实牵涉到大模型怎么能够让它的能力能够产生高质量的输出对 像语音生成它也是有情绪的这种你们怎么考虑你们自己做这一块还是说直接调用大模型的能力我们这回是自己做的就是从文本生成语音跟所有的表情动作自探是我们是自拍输出从文本到多模态输出全是我们自己做的你们之前的数据积累我理解其实就是外形的数据动作的数据表情的数据可能都有语音的数据也是有的一样有的对一样有我们自己其实是有一个studio里面可以采集各种的动作表情数据同时我们就自己的studio去录制这个最高质量的语音数据因为有的时候比如说我跟你交流的过程中我的语音跟我的表情动作是匹配的我同时要把这个数据同时录下来不如说我今天语音归语音动作归表情所以你录的时候这个数据就是要语音跟纯性是在一起的那你怎么考虑哪些环节自己做哪些环节接模型因为我觉得现在整个语音的应用包括模型在语音层的进化已经做得非常好了也有很多语音的开源模型跟接口出来是我们现在对于文本去生成语音动作表情这一趴我们自己一定会自己做因为我们现在这个特定的应用场景其实在现在的很多的语音的场景是没有的举个非常简单的例子我们在销售陪练的时候我需要有一个医生是某一种风格的或者我今天在做陪伴的时候他可能是一个某一种特定人设我们叫小奶狗比如说举个例子是吧那他的声音是特别了所以你如果只是去调用别人的声音没有去匹配他的人设我们很多场景是做不了的所以你们会针对一些特定的场景去做一些特定的声音训练比如说有哪些场景我们现在做了分B端给C端了B端里面可能会有不同比如说我做Customer Service的他的说话的声音然后或者我们现在做教练比如说我是一个销售的培训的教练或者我做面试官这些其实是B端都会有些不一样有些可能要求专业有些可能是稍微能够有一些严谨的到C端可能更会diverse一些比如说今天是做陪伴的可能会有卡通形象的陪伴他的声音可能像小黑一样萌萌的对所以我们在这一块因为很多的现在已有的TTS他的应用场景没有像我们diverse我们自己在整个做下的一个感觉是说如果你用通用的其实很多时候进到垂直场景是不行的你一定要做自己因为你不能等着他帮你来做我们就像一套套模板一样客户如果有文本大模型了以后我们就直接调用了整个纹身的多模态这一把就行了或者客户说我自己已经有语音了我不需要你的语音也可以你直接调用我的能力你都可以如果你什么都没有那我都提供对我们刚刚其实聊的很大的一部分都是在AI的技术如何去做虚拟的世界那反过来那你们现在训练的这个模型它可以去操控机器人吗你有试过吗我们试过我们在做所有3D数字人跟3D动画一个很好的点就是它能够驱动机器人比如说我是个3D数字人能跟你交流你问我的时候我能听懂你然后我知道用什么样的语音生成语音动作表情跟姿态对一个机器人来说我可以同样用这套东西去驱动它那机器人也可以做实时的语音动作手势只是现在机器人没有脸部所以他表情表现不出来所以他没有脸部的肌肉对因为现在机器人就是个男女那将来机器人如果做陪伴如果是做白领的工作比如说他是个销售他是个老师他可能也需要表情首先我知道这个机器人比如说我跟你交流的时候我的手势应该怎么动表情应该怎么动我的姿态应该怎么动那下一步就是说我们叫做用Imitation Learning就像NVIDIA那种方法我能够去做仿真直接能够拿驱动跟你做交流太有意思了然后你们现在在真实的应用中就比如说你现在用你的这个模型的数据接到机器人上去你觉得对它的哪一部分的提高最大因为我们可能说机器人是没有表情的对不对然后它的手势是可以动的你可以同时驱动手跟脚吗还是只能驱动上半身我们可以你们可以同时驱动手跟脚告诉你一个特别有意思的事情国内现在比如说我们合作的过程中因为我们生成的是从脸 手 包括腿部的动作其实全有现在很多机器人公司其实它建这个机器人的时候其实它的balance还没有做那么好就使得我给了它这个动作它可能也是用强化学习这家simulation去做因为我们提供了API给它们了以后那它们在这方面可能如果做得特别好的那可能也能够驱动起来那因为上身其实有很多的动作就是它要一定的泛化性但其实这个事情我觉得其实没有那么的难就像我们爬楼梯也一样我的动作能够通过我们的能力能够生产出来那我就在simulation环境里面加上强化学习让它能够去复制这个动作一点问题都没有所以机器人的平衡问题是我们收集的这些3D人的3D数据它只是动作姿态的但是它并没有力的反馈然后你只要加入到力这一点就可能会出现平衡的问题摔跤的问题我觉得你好专业没有没有我在尝试理解这里面是两个核心的点就是说你要驱动一个机器人一个叫做运动学我们叫kinematics还有叫dynamics叫动力学那第一步比如说我要抓一个杯子我首先知道我抓杯子手应该它的pose它的姿态应该怎么动去抓它那第二个事情动力学其实解决的是我要用多少的力能够按照我想的那个路径那个姿态去抓这个事情所以我们先做Kinematics但很多时候叫做运动规划做Motion Planning也是做这个事情的那一般来说两者之间可以结合起来所以我理解其实机器人公司在寻求合作的时候它两个都是需要的第一个可能他们自己现在如果从零起步做一家计程公司它最缺的就是数据然后你们有数据的模型就已经训练好了是因为我们聚焦的是交互从我们评类的角度来说下一步我们今年吧因为会发布一个3D动作的大模型比如说它可以爬楼梯或者你今天直接给它说你往前做五步趴在地上然后再爬起来再跑它就能自动生产出来3D的动作的数据这个动作数据当然可以用来做机器人整个的训练了因为它今天要去捕捉其实这个数据有的时候如果我们有这样的动作大模型了以后它也不一定要捕捉因为你捕捉也是获取这样的数据而已哦 了解因为我看波士顿动力的机器人就是这种爬楼梯旋转扮箱子都已经做得非常的成熟了但是他们其实我理解他是在大模型公司还没有出来之前这家公司在机器人领域就已经研发了很多年他用了各种的方式去做你现在其实是用AI的模型再去驱动的这一套就是爬楼梯的动作你觉得这两者之间我们看到机器人表现的是同样的技术啊爬楼梯但是他的技术路径是完全不一样的还是说相似的我觉得你刚才讲了一个很有意思的点就是波士顿动力他以前能做爬楼梯当然有一个点我想指出就是说他以前爬楼梯的时候他的泛滑能力并不强比如说你给他不同的楼梯高度是不一样的他不一定每一种楼梯都能爬得很好对因为他给你修Demo的时候永远给你修的是同一个楼梯对对对所以这里面有一个特别重要的我们叫泛滑型我相信今天做人型机器人或者做机器人大家都会讲到我的数据生成了以后我能不能做我数据里做不到的事情那这个里面就非常重要就是说爬楼梯吧每个楼梯其实有一个高度有多少层楼梯包括楼梯本身的摩擦力是多少摩擦系数是多少这个其实都是一些要犯法的参数那今天你有没有能力今天说给你任何个楼梯你都能爬得特别稳那另外是不是你能给我说我能控制它爬得快一点爬得慢一点这个事情其实在今天来说我觉得还是一个难的问题其实也是来自于数据假设说你今天有所有的数据比如爬各种楼梯的高度的那不同的快慢的那我今天去做这个事情其实也没有那么的难那我们如果现在做了一个核心的点我们说在虚拟世界里面包括我们后面要发布的3D动画的大模型其实也是要来生产出动画的数据让它爬楼梯所有东西都见过了哦 太有意思了所以你们其实也是在做机器人动作的泛化性你看一种机器人动作的泛化性我们也在做数字人动作的泛化性其实这两个是一样的你觉得用AI的方式去做机器人就是AI跟机器人它又经过了哪些变迁呢就像你说的可能最开始大家还没有想到要用AI的方式去做机器人后来又开始在AI中加入了强化学习去做机器人最早的时候AI机器人这个方向很难很难很难特别是对于人型机器人我们叫Biped就是两只脚它最难的问题是平衡那另外一个问题就抓取那个时候做人型机器人最主要是在日本有一段时间很火Honda叫ASIM那时候工程师要调一个他走路的动作你都不知道后面有多少工程师在调这个参数我们叫控制器怎么用这个力去控制它让它在不同的表面上能够像人一样走路这个参数而且也没那么稳定如果你把平面稍微改一改可能就跌倒了Learning的东西AI的东西其实是不多的那时候就走控制器所以早期机器人的发展它其实主要是控制就是为了让一个机器人不跌倒不跌倒Balance如果它能走不跌倒太了不起了那时候然后后面大家说光这么走不行啊你能不能就是有一定的泛化能力吗我走路的时候能不能在不同的平面不同的表面或者你的走路的速度都能够不一样这个事情如果你没有用AI的方法做你基本上不可能做的你觉得现在的机器人跟你当时学机器人的时候20年前进化有多少我的进化还是蛮大的就是以前让一个机器人有两只脚的能够跑走跑跳觉得这个是好难好难但你先看看看到国内很多做人形机器人公司运动会拿个遥控器控制它还是能走的嘛跑也能跑嘛大部分问题是能解决了那在这个事情在20年前基本不可能它的balance平衡太难了但它是通过远程操控的方式对 即是通过远程操控的方式它还是要解决我说的刚才的动力学控制那个问题我觉得控制这个事情如果你有视觉语言动作大模型那它就不需要拿个遥控器了但我们的小脑控制这个事情让它走不跌倒这个事情其实蛮难的所以当现在的进步就是说一方面是data一方面就是强化学习那么包括simulation这样的环境像V量这就是技术的进步能力开放出来了以后大家都在这个simulation环境都能去做你发觉其实也没有那么的难了是的那机器人走路不摔倒这个是现在一个做机器人的公司普遍能达到的水平还是说只有头部几家公司能达到对于稍微OK的团队我觉得是没什么问题的就已经都解决了但是有一个点就是你的犯话到底有多强你指的走路不摔倒是在他们日常的训练中特定的场景走路不摔倒如果新的场景保不定还是会摔倒那你觉得现在这个世界上能让机器人走路不摔倒在部分场景中实现的公司有多少如果说完全不摔倒在新的应用场景其实蛮难的我不知道现在有吗能够做到泛滑能力很强它能很卤棒不会跌倒其实基于我的认知来说可能现在还没有吧如果有那我可能要学习一下我觉得真的就爬楼梯这么一个事情我今天就可以设置任何爬楼梯的他肯定以前见过我不相信今天世界上有任何个人性机器人公司能够做到所以我们其实只是解决了在平面走的问题那个在特定场景特定的场景下包括你现在看还有一个事情是grasping就是抓东西抓取在我们那个时候比较早的时候用人心手去抓东西其实不多的包括机器人在整个的业界你去看很多时候它是个吸盘比如这个东西我吸它就行了但你现在可能很多人可以做抓取像人心手一样比如说拿筷子加东西那这个事情其实也是一个非常非常难的问题还是要大脑加小脑大脑首先得看到这个东西我应该怎么去抓它到后面用小脑真正去控制你的筷子去加它我觉得这个事情也非常非常难今天你现在看到的都是Demo而已我认为真正在一个特定的英雄场景下稍微有一点点泛化性但是如果把它最往外外延一些范花今天来说还是很难我们10月5号的这个活动你也去了嘛然后我们现场其实是有机器人给大家开可乐的头一天他们在彩排的时候中间我就放了一瓶可乐上去我说我也要试这个放完以后他说你得把可乐转一个方向他说那个环要对着他的手指如果你不把环对准的话那个机器人他的手的灵活度还很难去把他那个转一个方向打开是真的你这个还是在一个特定的环境它已经布置好了你更不要说它进入家庭那在这个家庭的环境中那更是各种各样灯光包括审舍限制那就更难所以我觉得这条路呢只是说大家现在看到了大脑我们叫VALA这个模型有可能可以去解决这个问题但是不是百分之百能解决其实也没有人知道如果可以解决到底要多少的数据才能达到一定的泛化能力一定的鲁邦性去解决大家只是觉得相信scaling law相信大模型这样可以有一天能够解决但这里面的挑战是很大很大是所以从你的角度你觉得现在世界上最好的机器人公司是谁为什么我觉得是这样做机器人它有不同的流派有做本体的比如说有做硬件的有做小脑的也有做大脑的我自己觉得还说不到有多好因为多好有不同的定义你今天在做研究你这条路已经有一些promising results还是说你今天已经落地了也许你今天这条路看上去很有希望也许最后发觉你这条路是死的你暂时的领先并不是最终的领先那你现在国内其实也不同的流派像语树可能做的是寄生本体加小脑他不是做大脑大脑是指什么大脑就是说现在做VOA叠衣服这种那小脑就是说你让他爬个楼梯跳个舞跑个步在这一棵里面有做本体的有做控制的就是说控制我们叫小脑就是运动规划运动控制动力学整个这一套还有做大脑的我觉得至少我现在还没有已经看到了曙光了可能我比较悲观吧它可能也会像任何其他领域比如说无论是VR AI也好还是AI领域也会有起起落落因为这是Robotics的第一波浪潮那无人驾驶其实也有起起落落但从长期的角度来说那肯定是promising的那从短期来说可能还是有很多挑战你觉得机器人模型要达到GPT30颗需要多久这个其实我没有那么强的经验或认知吧我觉得今天的数据要泛化能力至少要很长一段时间今天我看到的这些还没有让我非常clear地能够判断两年或者三年但我觉得十年可能hopefully这个问题可能能解决十年是很长的时间了所以这也是为什么你们公司在做的时候其实你没有直接去选我去切机器人这个赛道反而是说我们把3D跟机器人的交叉的领域数字世界里的3D我们来做一做如果我们讲迹数字人在数字世界里在ware space或者在屏幕上能跟人像人与人之间交流也是在这个数字世界里能够抓取东西能走路能爬楼梯那本身在数字世界里已经很有用了它已经可以有这个reward application了也可以有双业的落地的反过来如果这些做了以后对做robotics来说也是一个很有很有价值的因为我们在讲小脑这个事情的时候你的控制运动学动力学你现在也知道怎么动然后这次决定用签华学习用怎么样的力让他能够去做这个事情我觉得做research它是一个今天robotics是一个非常好的方向因为有太多东西可以尝试了如果从商业化的角度来说我自己觉得其实挑战蛮多的如果你要商业化落地至少从人形机器人的话我觉得白领可能会比蓝领更快是是是你刚刚提到在数字世界里面它可能也会涉及到一些力的反馈会有的比如说好莱坞动画里面我们把一个苹果一个南瓜甩过去它变成一个南瓜酱它怎么炸开那个就是就是物理其实物理还有一个点比如说你是个数字人或3D的角色你从二层楼跳到一层楼那你跳下去的时候你跟地面之间的整个的反馈怎么滚动一定要满足物理我们这个大模型的动画如果生成了以后它本身就可以用物理的方式让它在虚拟世界中去仿真它同样的方式其实也可以在虚拟世界中用强化学习的方式去生成这个控制器同样的方式我可以在实际世界中这么做本身的逻辑是很通的但我有一个问题如果我们把动画世界里的数据收集来学习我知道一个人从楼梯上掉下去以后怎么弹怎么滚的但是我不知道为什么我也不知道这个里面有多少地但是就是看见了这些现象然后用这些现象跟这些数据去训练出一个大模型它能反馈能模拟我们还是不知道力是多少就是我们说Skating Law跟这个所有的大模型都是黑核模型但是我们再把这个场景拉回到现实里面来我们要去让一个机器人砸到一个东西或者拿到一个东西这个力我不知道现在在现实数据中是不是就是要反复调控跟算出来的所以它就必须有这个力的数据我觉得人他真正在现实生活中我们去举一个杯子我们也不需要去计算它的力这就是我们的一个经验习惯就是我们有这样的一个感知就好了就是我总体的意思大概就是过去整个机器人他的研究包括他的力学的反馈还是在用白核的方式去做但是整个模型呢在用黑核跟一套更加经验主义的方式去做就是为什么我们觉得现在已有的方式上做的时候到真正你要泛化到一个实际世界中去抓取各种东西的时候它的挑战会非常非常大因为它的泛化里面的东西太多就是我们今天讲整个的过程中你在学利益的控制的函数以前的方式是要自己算的吗或者一个一个的自己对 现在就是用强化学习的方式去现在用强化学习就是大家不需要知道为什么跟这一条我只要有足够多的数据给它reward它就慢慢慢慢慢慢就能够做但这个问题是说我抓杯子只是我很小的一个例子这个世界上有多少所以我觉得说这个事情就是大家希望将来有一个基础的大模型我足够多的数据了以后大家能够去一个特定的场景我能去调优这个模型能够在这个环境里能把它慢慢把它做好但其实是很难很难你想我们学会抓东西我们学会走路跑步我们花了多长时间是的我自己听下来就是我对机器人领域这一波最大的进展我是觉得整个研究的方式它从白核模型的研究变成了黑核模型的研究从我们必须知道每一个细节它的受力点是多少靠这种计算跟一个一个细节调配的方式的研究变成了我们一个端到端的模型我们不知道里面是怎么运作的但是它可以是的我们在讲强化学习的时候就在讲这个事情这是一个最最大的点就是以前的时候那套东西就是更多是显示的比如说我要去扎这个东西我大概知道要什么利息以前算好不是说我在这个过程中我根据实际实际的情况表面的第一次抓的时候的感受我能够实际去调整它一定是过来挖的方向在后面这条路来说那一定是打开了以前对机器人来说觉得怎么做感觉也好像也没有希望那种感觉好难啊那种方法一定是不能scale up的但今天其实大家为什么觉得虽然我对一个外行来说那我自己会觉得很难你说13年还是5年但是从长期的角度来说是有希望的这套方法它在大语言模型也好在其他方向上也好其实已经展示了它的一个能力如果在机器人这个方向上如果你有足够多的数据是有可能能解决这个问题的它的天花板就是大家能看到是有可能有一天能走到我们真的想大家想象了一样机器人能干各种各样的活在各种复杂的场景里面那中间是不是在这个过程中会遇到大家想象不到的问题会还会遇到低谷其实我不知道看起来现在是一个大家刚找到一条新的路的那个兴奋感的时候但是它的结果是不是能收敛是不是一直能看到效果是这可能就是中间起起落落的过程是是的是的好那谢谢柴教授好谢谢好的那这就是我们今天的节目很可惜啊就今天我们在录制的时候是播客不是视频所以其实很难去实时地展示跟感受一下这个3D数字人的效果那也希望未来我们视频可以出一个成千的数字人给大家看看感受一下所以如果大家对比如说像3D的AI数字人到底可以用于哪些好玩的领域去做一些哪些好玩的事情有更多的想法欢迎给我们写评论多多交流那我们的播客听众可以在小宇宙苹果播客Spotify上来收听订阅我们如果大家想看字幕版也可以在YouTube或者Bilibili上来订阅我们未来呢我们也会推出我们的newsletter在硅谷举办一些线下活动如果大家对我们的线下活动感兴趣啊可以在我们的show notes里面订阅我们的newsletter我是红军感谢大家的收听优优独播剧场——YoYo Television Series Exclusive
