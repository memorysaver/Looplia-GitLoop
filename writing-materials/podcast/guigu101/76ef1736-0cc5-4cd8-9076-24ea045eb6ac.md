---
id: 76ef1736-0cc5-4cd8-9076-24ea045eb6ac
source_type: podcast
source_key: guigu101
title: "E145 | 对话Meta田渊栋：被Transformer改变的世界与人类AGI的野心"
url: https://sv101.fireside.fm/151
published: 2024-03-28T23:30:00+00:00
downloaded_at: 2025-11-28T16:13:20.316288+00:00
---

# E145 | 对话Meta田渊栋：被Transformer改变的世界与人类AGI的野心

请不吝点赞 订阅 转发 打赏支持明镜与点点栏目大家猜一猜中国有多少只有28%所以说啊中国的算力资源虽然现在是稳居世界第二仅次于美国但是呢其实现在这些算力的利用效率并不高其中一个原因就是龚友云这种高效的算力模式占比过低那之前老罗他其实自己也带火过很多的概念我们这次来看一看老罗能不能引领一场企业级的IP认知运动把云计算这个概念推向大众那他这次的选品也是覆盖了阿里云众多的热门产品价格也是给出了史无前例的优惠所以大家如果感兴趣的话可以去淘宝APP搜索罗永浩3月31号晚上7点让我们一起围观连续创业者罗永浩卖云产品以及它是如何解决创业者的核心痛点的2017年谷歌一篇划时代的论文Attention is all you need先开了这一轮人工智能的开幕式这篇论文也是大家现在知道的大名鼎鼎的Transformer七年过去了我们看到有人在这篇论文上加算力加算法开启了第三次的科技浪潮今天我们的嘉宾来自Metalfield的研究员田渊洞博士他最近也发表了两篇论文都与端册的小模型相关由于离应用更近并且在解决更加实际的问题所以他的论文经常被很多工业界的人问到而过去的这些年他所有的研究都在回答同一个问题就是神经网络是如何工作的今天我们就一起尝试探索这个问题也跟大家一起聊一聊最近大火的Sora,Transformer,还有AGI哈喽田博士你好你好我大概跟可能还不太了解田园栋的听众简单介绍一下他过去的经历庭圆栋是2018年为期开源项目ELF Open Go研究及工程负责人和第一作者当时我记得这个项目其实在开源社区里面名气还蛮大的然后也是曾经获得了2021年国际机器学习大会ICML杰出论文奖提名以及2013年国际计算机视觉ICCV马尔奖提名它的研究方向是深度强化学习表示学习和优化历任机器学习国际会议ICMLNeuropsAAAIAI Status领域的主席而且我记得你是2013年到14年左右是在谷歌的无人驾驶担任软件工程师是的那个时候应该是无人驾驶非常非常早期的时候整个业界还没有注意到无人驾驶的时候对大概是这样子吧或者确切说是国内还没有注意到在硅谷这边其实已经有人注意到了就是有挺多的机会吧大概在2015年16年开始就起飞了对我当时有很多人来找我非常非常多的人来找我我说我不去还是在这原地今天我特别想跟你聊一个话题我们何时能实现AGI非常巧的是我之前在知乎上看见你写过一个关于自动驾驶的帖子我觉得你那个判断跟之后我们讨论的AGI某种程度上会有一些相似之处我觉得待会我们可以详细的聊一下你为什么放弃了自动驾驶行业但之后我记得其实在2017年的时候你刚刚就我们提到的这个围区开源项目ELF OpenGo也是基本上当时人工智能界最火的事情因为当时谷歌的AlphaGo AlphaZero其实是大家看到了人工智能的一个大的突破的当时你也是在研究人工智能怎么下棋的这样的一个问题对 1718年的时候我们也在做AI4 Games就是AI4游戏这样的一个方向其实这个方向很早就做了大概2015年的时候我们就开始做我刚刚进Fair的时候这个项目就是我的第一个主要的项目对 做围棋当时我们做了一个Bot叫Dark Forest黑暗森林Name after刘慈欣的三体的第二部当时我是三体的粉丝当时是在Offergo出来之前对吧我们的Bot还是比较强的能够跟当时最强的软件打个差不多平手吧当然没到职业水平但是还是很强不错当时我们也是用神经网络做策略做policy network做策略网络得到一些比较好的结果就是他们能够跟当时的最强的那个软件他们花了十年打磨的水平差不多让我觉得非常惊讶其实你提到了你做Dark Forest的经历我想到你还有一个身份就写《杠七年》你其实也是一个科幻作家对对对我从06、07年的时候应该说更早吧05年的时候开始写的当时写的非常非常非常烂你看到网上的小说那些都不是一开始写的后面有空会写一点小说吧我觉得还挺有意思因为对我来说就是不靠谱的想法就扔进小说里面写靠谱的想法就来做科研我觉得这样其实是一个比较有趣的一个组合吧因为我也想过很多时候想法但是不可能每个想法都自动能达到一个就是能发文章或者说能够谈生意想力的这样一个结果对吧所以有一些非常非常那个crazy的会上天马行空的想法就可以把它放在一起也许就有一天就会有一本小说出来对 其实我挺好奇的你研究过自动驾驶研究过夏威奇包括当时你也做了Dark Forest的策略的研究我挺好奇现在你是怎么转过来研究大模型的而且最近你们团队很高产对 这个中间也经历了很多的过程大概18年我们的OpenGo出来之后19年发了文章之后其实再往下这个方向就没有再做下去一个是因为团队里面不同的人有不同的想法OpenGo的最后一作Larry他现在做AI for Science他有一些关于那个AI for Science的一些方向比如说Chemistry像他们的Catalyst这个Project那么每个人想法不一样有些人已经走了嘛所以后来也就没有再继续下去但对我来说我当时其实有很大的一个初心吧想要去理解神经网络到底为什么工作他为什么能工作那么好就是关于神经网络的理解当时我就在想其实我应该做这个所以我在18年19年之后呢就没有再做围棋而是去做这方面的工作所以就是说如果你看我这边的publication list大概从19年开始20年有很多的我一做的文章是关于如何对神经网络有理解为什么它能够搞得比较好那么这块其实我们做了很长一段时间其实当时19年的时候OpenAI的Elia他其实跑过来找过我的他跟我说有没有兴趣当时我说加入OpenAI他当时向你发出OpenAI的邀请对我以前背过一个包就是YC120的包那个包其实是当时我参加他们的活动他们送给我的他们当时有跟YCombinator有一些活动当时伊莉亚是请了我去了一次问了我到底要没有兴趣当时跟我说的是他们想做Language Model大语言模型说我想做的是如何理解神经网络为什么能够walk然后当然两个人谈不拢了所以我就没有去当时是有这个有趣的一个故事吧当然最后大约模型是起飞了都是在2022年的时候2022年的时候起飞那时候就是真正的我就感受了确实这个方向非常非常的有希望同时呢这个方向其实也可以跟我这边的一些对神经网络的理解其实是可以结合起来的去年我们发两篇关于如何理解Transformer的工作一篇叫Skin and Snap还有一篇叫Drama两篇都是我一做其实就是说把我这个如何离开神经网络和大元模型结合起来了因为Transformer现在就成为了新一代的王者很多很多网络很多很多神经架构现在都开始用Transformer来作为这个方向对这个东西进行理解呢其实是一个很重要的一个问题同时也可以通过这个可以得到更多的一些对于神经网络的改进的想法和建议像这次的两篇文章一篇是Mobile IM还有一篇是Galori这两篇其实都跟我们之前的一些对神经网络的理解和分析是有很大的关系的Glori现在还是很火前两天我们发了之后网上有很多很多的反馈包括我的微信也有很多人找我问一下接下来我们怎么做啊或者说我们对这个东西的看法是什么样子的已经有开源的社区重复了我们的工作对对对而且确实确认了我们的一些发现比如说怎么省内存啊确实可以跑下去速也还可以效果也不错所以这个让我非常开心那么这篇文章其实有很大的一个motivation呢是源于比如说2020年我们的一篇做网络分析的一些文章那么这篇文章当时被拒稿了被哪拒稿了当时被I-CLEA拒了同时也被ICMR拒了别拒两次之后就没有在投因为已经灰心丧气了就一直放在archive上但这篇文章的两部分后来起了很大的作用有一部分呢变成了去年我们这篇Drama的一个文章的分析的基础另外一部分呢它的分析的能力和他的一个思维方式呢就变成那个Garrow这篇文章的一个主要的foundation所以我觉得这很重要就是基础研究对于整个领域的一个促进作用还是要很后面才能看出来的要过个四三到四年的时间对 你刚刚的这一段话里面我有太多太多好奇的问题了我觉得一个一个来案首先是你提到了你的Garrow的这篇文章我知道这篇文章确实在行业里面引起了非常大的反响可不可以这样简单通俗地跟听众解释这篇文章其实你主要讲的是怎么样在一个24GB的消费级的GPU上去运行一个GB模型的可行性可以这样理解吗对 这个是一个但是呢 改肉主要的focus呢还是能不能在4090的卡上去训练训练对 推理呢 其实很多已经做到了如果有一个模型大公司或者说大团队帮你训练完了你可以把它弄下来然后可以在手机上可以在你的个人电脑上进行推理这个不是新鲜事大家都在做而且已经做得很不错但是如何能在比较小的消费级卡上能够做训练甚至是预训练就是从头开始训练那个其实是之前一直可以说是大团队或者说大算力的团队的一个独有的一个能力对于消费级显卡来说或者说对于GPU铺这一组来说就是比较难但是这篇文章可以让大家看到了一个希望如果有很多的这样的卡连在一起的话也许也能做到大模型预训练的一个效果当时就是我们现在预测算下来就是单卡4090训练一个7B的模型肯定要110天非常非常长对吧但是也许如果你有很多卡的话那也许会有并性因为我们现在能够把整个7B模型放进24GB里面的话那也就意味着模型之间的不同层的一些交互其实在单卡里面可以进行就不用出现跨卡交互高带宽交互的这样一个情况那么这个其实会大量的省带宽如果你有很多4090的卡的话也许他们可以通过PCIE或者说通过A3NET就能够联系来然后他们能够训练完那么这样的话就有可能会发生这样的事情我跟大家解释一下4090是英伟达给大家用来玩游戏的显卡它很难跟A100 H100的算力相比但是它也是一款可以说是性价比非常高的卡简单来说是大家在训练的时候可以更节省显卡了对 这件事还可以绕过以前的一些限制比如说为什么现在显卡那么贵主要一个是NVIDIA现在有NVLinkNVLink是有很高带宽它可以提供很高的带宽是因为一个模型的不同的部分放在不同的卡上所以它每次在训练的时候它们这件事要交互的这个交互其实是有很大的带宽要求比如说几百GB每秒的速度交互但是如果一个模型的所有的权重都能放在一张卡上那么也就意味着它们在进行梯度迭代的时候内部的计算可以在这张卡上进行卡和卡之间的交互就可以大量的节省那么这样的话也许有一些先当前的一些范式就可以发生改变你觉得哪些范式会发生改变比如说现在有很多的model parallel的模型并行的一些方案像FSDP这种方案你要保证模型的一部分放在不同的卡上模型太大了比如说需要大量内存比如需要100G内存但是你每张卡只有40G或者只有24G你怎么样把内存分配在不同的卡上把权重分配在不同的内存上然后让他们充分的高速的交互让这个训练变得很有效率但是如果有我们这个方案之后呢也许一张卡上可以放更多的权重这样的话呢训练的这个过程呢就会加速你是怎么做到的我们在算法上做了一些改进关键的一个点呢就是说因为大家都知道Laura权重太多了那么一个资料的想法是我们把权重经过重参化Reprimetization然后把权重变成小的矩阵相乘这个叫低质矩阵分解分解之后权重里面的参数的数目变少了那么这样的话我的训练就能够把内存的要求降下来这是LORA的一个思想LORA是有问题的问题在哪里呢就是它一个它不能用来做预训练特别是在一开始的时候LORA直接预训练会导致爆炸会导致有各种问题就是训练出来的效果肯定没有全参数训练要好那么这样就导致一个瓶颈就是说你要省内存你就不得不牺牲一些性能那么我们怎么做到呢就是说我们的观点是权重本身不应该是低质的不应该是low rank的但是它的T度的迭代这个T度是可以是low rank的这个是不一样的对对因为其实大家可能都想的是权重本身是有这个形式其实权重并有这个形式但是我们可以证明甚至可以证明出来T度的迭代呢T度本身是low rank因为T度是low rank的话呢那T度对应的一些相应的内存开销都可以是low rank的包括Adam的一些状态它的momentum有些variance这些东西都可以是罗软科的那么这样的话就一下子降了很多的内存那么这样的话甚至是比如说你要训练一个7B的模型如果你用一般的方式训练的话你知道要40GB以上的内存你一个7GB的卡也没有放不下但是用我们的方式的话我们就可以把它砍掉砍到一半比如说18到20G就可以放进去那么这样的话就会让4090比如重获新生比如说重获这个方式在算法上可以改变计算的过程可以让这个训练变得更有效率更深内存非常简单粗暴的理解它其实是一个算法的改进而不是说你加进去的预训练数据质量的提高是的数据是另外一回事情这个改进跟数据是平行的关系或者说是slogonal一个关系那么数据还是可以再往里面加那么效果可以更好但算法本身如果效果更好的话其实跟这些数据的改进是叠加的关系这篇文章的目的不是说要训练出一个跟现有的算法相仿的7B的model我们这边实验就跑到了20个billion的token但是算法如果你真的要训练一个比较好的7B model至少要一个锤链或者说反正是三到五个锤链或者两到三个锤链这个数量级所以现在还没有到这一点希望以后如果有机会还可以再往下做下去你还有一篇论文是Mobile LM要不要跟大家简单解释一下这篇论文的主要思想这篇论文就是说是我们能不能把全球网络训练的参与数量压得更低我们这边用的是350米链的更小的神经网络那么在这很小的神经网络下我们是不是能够训练出更好的模型这个模型的能力肯定没有大模型小那么好但是小模型到什么程度这本身是一个很有意思的一个问题那么这片呢其实主要我这边是作为一个Advisor这样的工作因为这片主要不是我们Fair组的工作这片是Reality Lab他们组的工作我作为一个external advisor给他们一些建议包括层和层之间其实可以共享参数这些建议这个都是当时我们讨论出来对我稍微穿插一下那个问题因为我一直都很好奇我们何时会达到AGI其实业界有两种观点一个是仅仅通过增加模型的规模通过scaling law的形式实现AGI那其实还有一种观点仅靠扩大规模它一定会遇到瓶颈的这是它是不够的你更倾向于那种观点一般很多的这种问题我永远会倾向于我觉得我们现在离AGI还差几个breakthrough我觉得现在直接scale不一定能有效果而且我会觉得可能真的过一阵子大家会发现scaling慢慢它给你的benefit会越来越小因为scaling law是什么scaling law本来它其实就是一个promise就是如果你的算力乘以2数据乘以2各种东西乘以2那么你的performance一定会上升一个固定的百分点但这个其实不是一个好事情New Network可以有这个Skilling Law但是同样的Nearest Neighbor最近领方法有检索方法其实也可以有这个Skilling Law一样的就是说数据量越多Inperformance越好所以模型都是这样子所以其实我们并没有完全理解为什么现在的Transformer或者说Largetle Model这样的一个方案有那么好的效果其实并不是很理解而且这个Skilling Law走上去之后意味着这跟制动价格是一样的他们最终会达到这样的一个Curve那么这curve是什么意思呢我当时在自动驾驶2017年的贴子上这么画过一开始大家都非常非常的激动说我加点数据一下我觉得那么好了这个人类就马上就要领来新的春天了但是最后你会发现数据加的越多它的performance提升就越难以被人发现然后最后它离人类最后跟线可能还差一点最后可能会有这个问题那么对自动驾驶来说它的这个问题是数据会越来越难获取主要问题就是在这儿因为如果你开一个100万卖都没有任何交通事故的话那也意味着你每100万卖才能收到一个数据点反正这个效率是非常非常低的这样的话你这收入效率越低那最后的结果就是你永远达不到人类的水平那么对大元模型来说这个情况会好一点因为数据很多时候不需要那么从各种事故中搜集还有很多的数据能在网上能够用到但是也是有同样的问题也许我们会发现比如说再过10年有90%或者80%的日常的一些行为我们都可以把它们建模建模的很好的但还有20%或者10%这些行为因为它是可能每个人独有的或者说是私有的那么那些数据你会拿不到的你会拿不到的话那也意味着就是拿不到那我就没有办法用它来训练模型那么模型就不会真正的去理解这些情况那么这个其实是一个很大的一个问题这当然是一个了然后另外一个就是说大家可能会意识到数据会变得越来越重要那大家也不太愿意真的把数据分享出来这个两方面的因素就会导致有问题就是最终可能会发现就是数据越来越难获取数据难获取之后你的模型就会变得越来越难变得更强所以这两个其实是有一些一致性的所以我觉得而且你就回想一下对人来说比如我跟你对话我能很快的去理解你的处境你的状态那不需要你的大量数据就能理解但是机器就不一样所以这种情况下其实人在这方面还比机器要更多一层的理解能力和深度在这方面其实我们现在还没有看到其实刚刚你分析自动驾驶的这个思路特别好这应该是我看过的最好的一个对自动驾驶非常独立思考又用数学逻辑讲何时能达到完美的我说的是完全的无人驾驶的这种状态的一个分析思路所以你觉得这个思路在DGI领域也是存在的对也是存在当然自动驾驶有它的特殊性因为宋驾驶一个是100%可靠完全不能犯错那个要求其实比大元模型要强多了大元模型没关系你做错了大家想想就好了他还是能帮你改稿子还能帮你总结各种文章还是能帮你提供各种建议所以他这种属于更多像是创新型的如果没有什么好建议没问题没有损失有了好建议是更好但是对宋驾驶来说如果没事是你的expectation有了事就是你的问题这个是不一样的对我理解你是更倾向于第二种观点的其实AGI不是Skilling Law它可以无限地增长下去而是它一定会遇到瓶颈在这个瓶颈中怎么样依靠其他的我们跳出现有的技术范式来去实现它反而应该是值得思考的对 我觉得是值得思考第一个依据呢就是说人类在从小到大的过程中它其实没有那多数据给它只给它喂但它还是做得很好你看我们女儿怎么样学会各种技能你会发现他学会的这个过程一个是不需要人工干预二是他学的速度是非常快的你可能昨天他还不会攀这种小岩石今天他就会攀了他大概几个月的时候给他放在楼梯上他从来没见过楼梯他十分钟真的就会往上爬这个其实是非常impressive也许有人会说很多东西是课在基因里面的这是有可能的但是他的学习能力也是非常非常强大应该是存在一个新的学习算法而且这个学习算法应该是远远超过现有的效率我们现在只是摸到了一个就是scratching the surface就是我们现在只是摸到一点点我们通过一些奇奇怪怪的组合运气很好的碰到了一点点的皮毛我们就已经打去那么大突破了所以可见这个东西的潜力非常非常大所以你在做Glero的模型的时候其实你已经是在思考这一层的问题了怎么样在算法上的提高让它的预训练更好对这个不仅仅是Glero这篇文章嘛应该说这个是go throughout the entire research career因为我整个research career一个很大的核心就是我怎么样去理解神经网络是如何工作的根据这个理解怎么样找到更好的算法提高它的工作效率这个其实是一个最大的一个方向这个方向上我们有很多的一个是理解一个是分析然后用这个理解和分析我们应用在现有的算法上把它变得更好比如说围棋包括之前我们其实还有一篇文章也是听过的叫Search Former对我知道那个对那篇其实也是花了一些时间去做这篇其实是Transformer现在的问题是他没办法做大长城的推理他的推理力非常差比如说让他去玩紧自棋你发现他不行我也试过包括最新的Crowl3玩一下发现也不行他说得非常好但是他下的棋是错的我跟他说我应该下这个下这个我们就赢了他说好像你是对的他还是没有办法做这些非常简单的一些比较复杂的游戏式的推理那么我们的Search Formal这篇文章呢通过先模仿传统的推理算法优化算法planning算法的过程我们可以达到一些水平所以就是说我们一个很大的关键的一个思路就是我们能不能找到为什么神经网络能够walk的原因然后能够用这个去理解去改进现在的算法你刚刚提到了其实你一直在研究神经网络到底是怎么工作的我自己对这个问题也非常的好奇那我把它具象成一个更加具体的问题我知道你研究Transformal研究得非常久而且非常深你要不要先说一下为什么在这么多条路径中包括像OpenAI它的训练它是用Transformer的架构来训练完成的它的优点是什么它为什么走出来了它现在的瓶颈跟缺点在哪里首先OpenAI用Transformer不是他们的原创了因为Transformer一开始是从Google里面出来的我觉得他们一开始的想法应该是对GPU的计算那些深入理解因为Transformer的一个好处是它可以有很大的并行能力它比如很长的序列我可以用safe attention这个机制同时计算这个序里面所有两对的token它们之间的一个similarity然后用它来算attention那么我觉得这个原因可能是因为Google内部做GPU或者做TPU可能发现算力的价格远远比通信的价格便宜既然算力那么多那为什么我不能设计一个模型让这个算力获得更大的优势那好我们干脆就把所有的token放在一起然后我们让他们做paywise的那个inner product这个应该是他们的motivation他们发现效果特别好速度也没有慢太多所以他才有Transformer这样子的但是呢其实后来发现Transformer的scaling能力非常好往里面为数据就在数据多的情况下它的效果确实比以前现有的一些方法要好像CNN那些方法它确实有在一些方面上不一定比得过Transformer因为CNN有一些预设的立场它比如说第一层感受也非常小这个对Vision是对的我先把最小的一些特征拼起来然后再拼起来但对Transformer来说它没有这个预设立场那么就意味着数据多的话它这个效果就会有更好的提升它相当于是一个典型的用算力来换预设立场的例子因为你要看人类社会的发展或者人旧的发展的话一般是这样计算资源比较低的时候就需要人类的大脑去想到一些比较好的模型在这模型上找到最好的解但算力越来越多的情况下人类的一些调的框框就要被打破了慢慢的人类把调的框框打破之后把这些模型的建立的方式让计算机自动去发现那么这个可能就是Transformer一个在宏观上的一个很大的故事吧对缺点呢对缺点呢当然是它需要打量算力还有一个就是它的速度也没有那么快就比如说你要在车上你用Transformer这个其实是比较难的就是如果你要坐无人车或者说做这种low latency的application它延迟太高它延迟比较高这种时候呢很多人就说你还不如就用CNN ResNet它的效果还是不错的也有很好的低延迟的一个特性然后它算力也不需要太多这个是一个很大的一个difference这个都是有tradeoff的但是Transformer作为头牌作为AGI的希望对吧肯定大家还会继续往下挖下去对我为什么刚刚花那么多时间在Transformer上啊是因为我其实在想我们达到AGI的方法是不是Transformer这一条路径因为我知道现在也有很多像新的路径在做比如说像Rakku但我知道他可能在处理图像跟视频的并行计算上可能没有Transformer表现得那么好刚刚其实我们也提到了仅仅通过增加规模你要最终实现AGI是很难的所以我们要找到新的范式所以我不知道你有没有发现一些新的范式整体上我是在想这样一个问题就是在做学术跟科研的时候什么时候大家应该是在一个小问题上不停地精进优化就是我们把各个细节做到更好同时另一个方面就是有的时候我觉得大的突破它并不是说在改进细节上做出来的而是说think out of the box就是我们怎么又跳出来从本质上质疑这件事情来达到的这是很好的问题就是exploration和exploitation的问题强调学习里面一个根本性问题一个是说我什么时候应该做探索我什么时候应该利用现有的优势获得更大的利益这个永远是一个好的problem我觉得对于研究员对于研究员来说很多时候是多线并行的不是说我就盯着一个东西因为你谁也不知道将来会发生什么事也许Transement会继续霸榜十年或者说明天他就被一片新的archive干掉你并不知道将来会发生什么样的事情对我来说我觉得大的逻辑就是我们还是要从第一性原理出发这是我的一个风格我永远很非常喜欢从第一性原理出发就是OK我对于世界上所有的人说的话我都不一定会100%相信我们相信的是什么呢是如果我们要分析如果像一个数学家一样从最简单的定理出发OK这些数据之间是有相关性的什么样的模型能够很高效的去模拟这个相关性把相关性提取出来用这个相关性去预测将来的一些事情有这些东西之后那么你就会自然而然会构建出它整个框架应该是什么样子的根据这个你再去找很多文章去验证就我每看一篇文章我不是说是真的去百分之一百把它们的所有系列都记住这个没有意义因为那么多文章根本看不完我觉得我做的很多事情是这样去看这篇文章然后去看这篇文章里面哪些观点或者哪些现象和我心里面想对神经网络对Transform的理解是不是契合或者什么地方是让我觉得我的理解是错的我应该改变我的想法通过这个方式来看文章最近有看到什么好的观点吗没有特别surprising的吧现在对 现在还是说在这个方向上我们继续往下去像Sora是一个很surprising的direction这个是一个很有意思一个就是这个效果就是比我能想象的要好好不少但是也有很多的文章比如说有些文章跟我们的研究方向是很契合的那我们需要去看一下这些文章能在做一些什么事情比如说能够加速推理的那些文章我们去看因为我们去年有一些文章是加速推理的神力存的文章或者说一些对于神经网络的分析的文章我都会去看一下那么这些文章其实跟我们研究方向是有关系的他们号称比如证这个东西能证出来或者说有些新的现象和观点那我也会去看一下刚刚你正好提到了SORA我们也还蛮关注Sora的你觉得它让你最惊艳的点是什么首先第一个就是我这边不是做困难模型的我很早以前做转机视觉但是现在也不做很多年了可以这么说吧我已经不是一个一线的在Sora方面是一线的我肯定不是我当然可以给一些建议或者给一些想法我觉得最爆的点就是它确实效果非常好最让我觉得surprise就是它一致性非常好你说的是所有的生成现在市面上放出来的那些还是说它的某几个demo应该说所有他生成出来的东西实际上都非常好这个不是说是一个两个比较好但是我觉得基本上所有的拿出来的东西是非常好的都是一个完整的场景然后场景的前后人物的表现和他的那个穿着还有各种行为都还是比较相似非常非常consistent这个是非常强的这个是为什么让我觉得非常surprise的原因然后你去看他的据据报告你可能会发现他并不是预测下一针通过这个方式预测出来它是它能够把整个视频看成一个大的image然后它有像这个3D的一个image这3D的image我们通过diffusion mode一点点diffuse出来那么这样的话确实可以保证一个consistency因为整个东西是一起出来的它不是像predict next token一样就是我predict一个两个三个四个然后我predict比如说60帧然后慢慢predict100帧慢慢慢慢把这个视频生存这个会有问题因为你预测到最后你慢慢慢慢会发现有些人就走行了或者说有些一致的概念不一致了然后他会去别的地方但是Sora在那个Latent Space上做这个diffusion它对整个Image 3D的Special Temporal Volume做diffusion把这问题提到了一个新的高度保持一致性有多难或者说它跟时间是有关系的吗因为比如说我们看像Runway像Pika他们是3到4秒最多extend到10秒的视频我看了Sora的demo它只有在东京街头那一个视频是60秒其他的视频可能也是20秒左右还是10到20秒之间还有8秒的对吧他并不是所有的视频生成出来都是60秒的但是我想知道保持一致性你看的是一个多大的时长然后保持这个时长的一致性有多难我觉得20秒内它的各种变化是非常大的比如一样有一个还是让我非常impressive的就是那个反光就一个人在车里面像东京的金甘县的街头就是他站那边看外面的场景不定期会看到外面但不定期看到他自己的反光他自己的反光在不同的时间都是一致的这个是一个非常让我非常surprise的情况因为如果你只预测下一针的话你很可能会发生这一针你预测出来的反光下一针反光就是不一样的但是它能做到两次的反光是一样尽管这两个chunk比如隔了10秒钟他们还是能保证这些反光是一样的所以这个其实是让我非常surprise的一个点20秒或十几秒其实倒不是问题问题还是在这里面的视频里面有多少内容这个内容是不是在经过很大的变换之下它还是能够一致因为我看到很多这种镜头翻得非常大整个人在很多的就是之前有一个视频就是一个猫在一个废弃的垃圾箱走来走去它整个走路的过程是变动是非常大的视角变换也非常大它整个猫的形态还是没有发生改变所以这个是非常impressive所以Sora是世界模型吗这个是一个很好的问题因为世界模型这句话是非常广泛的对我们先定义世界模型对什么叫世界模型就是你只要预测下一针或者说你对将来有一些看法你都是世界模型三十小孩说今天晚上我要去外面吃饭其实世界模型一样的所以我不觉得这个词这么高大上SOLAR其实你可以用来做世界模型对吧你可以说我把前面几针定下来给定前面几针之后把后面的东西拿过来做Diffusion然后得到一个Consistent的Video这个就是一个世界模型这个都可以而且SOLAR可以做反过来的世界模型我记得他有一个视频是给定后三分之一的针生成三个不同的试试品它们最终都会收敛到最后后三分之一的针这后三分之一的针是说一个电车开进来的三番的闹市区但是开的过程可以是不一样的它可以说这个电车先从空中降下来进闹市区或者说从另外一个地方开过进来最后它都会无份衔接到最后的开进闹市区这个动作所以去说SOLA可以做任何的补权了你去掉一些针然后把瞎针补权它都可以做对所以这种正常上说它确实是个世界模型但另外一个问题就是说这点上我同意一样的观点呢就是它在物理上是有问题的很大的问题一个比如一玻璃杯掉下来之后摔碎了但是没有摔碎的过程它直接会变成碎掉的状态是因为数据的问题吗就比如说玻璃杯掉下来大家能看到碎成一个渣子这个是经常我们在图片或者视频中能看到的但是它掉下来摔碎的这个过程其实是我觉得在人类的生活中它也是很快发生的我们不太容易去捕捉到它的是的我觉得其实就是这个原因就是说一个数据不够然后另外就是说这个物理过程非常难模拟在机器人那边其实有同样的问题你机器人那边其实也要做世界模型对吧你要预测下一针这个物体会在哪它跟其他物体有什么样的交互预测物体在哪非常简单因为你只要套你的物理模型就行了你把牛顿的定律拿进去套一下就好了但是它一旦跟别人物体交互的时候就会有问题因为交互的动作它的变化是非常快的交互的数据也不是特别多最后它学出来的模型质量就不好但质量不好的问题就是而且模型变动一点点它最后的输出也是完全不一样的这些因素综合在一起最后导致世界模型或者特别是交互比如说一个手砸在这个桌上这个交互这个交互其实是很难的因为在砸下去的一瞬间你收到的力从零突然变到很大这是非常小的在很小的时间里发生很大的变化这是一个两个物体之间相互交互比如说你要看两个人打斗我觉得这个对Sora其实会比较难我看见Sora它有一个是两个船在一个咖啡杯里面运转那个也是很惊艳的对那是很惊艳的那个就是说船和船之间是没有关系的船和里面的水是有关系的对所以它也是会涉及到力学的它会涉及到一些这个我相信是因为网上有很多这样的视频就是有很多水跟物体的模拟有视频那么就会学到这个能力但是如果数据不够的话可能就比较难那我把我的问题稍微再拆解一下我把世界模型这个词拆解一下你觉得Sora现在是不是能够理解世界运转的规律把握物理法则记忆检索信息还有逻辑推理或者行动规划的能力我怎么感觉上把Sora当成一个全能量模型我觉得它一定要分嘛不同的你说你觉得它那些方向做得好其实挺难的因为生成视频和预测物理世界是两回事因为甚至是一个视线而非的看起来很有道理但其实是不对的是非常非常正常因为物理世界对于视频来说最重要是好看最重要不是说是它真实对有可能会这样就是物理世界上有一点点发生小小的变化这些小小的变化真的模型没有预测到你的planning计划统筹能力就会变得非常差因为这个我觉得它planning跟这个能力应该不会特别擅长因为有很多的小的物理误差它累积起来之后就会变得非常非常糟糕我觉得这个是一个问题但是逻辑其实很难说吧就逻辑逻辑的模型你Sora更像一个创作模型它可以创造出一个视频对它来说是最熟悉的但是如果你要考它一道题这道题它没有见过或者说它不太有把握它不一定的回答得特别好所以我觉得这个其实很难不要把它当成一个万能的一个东西我觉得它可能确实是往前走了一步但是真的你要有万能的模型的话你可能还需要很多很多的工作其实我们过去提到大模型大多数时候说的都是大语言模型但我知道严罗困他有一套理论他的理论是全脑模型人体他觉得不仅仅是语言他觉得可能也要用感官去认知世界你怎么看这两个其实我觉得这个也可以算是我们刚刚讨论的到底是thinking in the box or thinking out of the box我觉得对于样来说他当然是希望提一个很大的大框架包括里面所有东西都有用感官就是Brain and Grant Perception或者说Embodied AI用感官去跟这个世界做交互然后得到一些信息那这也是很重要的因为对于人来说如果就给他看幻灯片看电影的话他其实就他的学习或者他的工作他的进步不会有太大的反应响那么对于人来说最重要的还是能够跟物体性交互这个就是强化学习这部分内容就是说我看到一张纸我可能想会翻来翻一眼未来会翻我可以想知道就是有一些新的假设你从你脑子里冒出来你要从探索中把这些假设解决掉我觉得这也是很重要的这也是ExplorationExploitation这样的一个很重要的一个点那么世界模型其实是其中的一个很重要的一个主存部分因为你对这世界没有预测的话你是没有办法得到你想要做什么事情的一个决策的就比如说你看到一只老虎你第一反应是老虎要把我吃了那我就得跑世界模型这边的决策是预测老虎会把我吃了如果我不动的话决策模型会是根据这个预测会决定我们要跑路这都是相母相成的对 所以整体来说它其实还是一个非常复杂的工作机制是的 是的 是的然后另外大家关于SORA讨论的比较多的一点它在做SORA的生成的过程中用了很多的合成数据包括用到了虚幻引擎5吗这个它没写它没写吗它没写OK这个我觉得有可能是因为误传吧因为SORA刚出来的时候有几个人在推特上写了一些猜测包括我自己他写过猜测我说他一定是他是不是一定有引擎它好像很多照片从引擎生成的当时我记得Somis好像也有一些想法就是Petrox的一个还包括Jim Fan它有一些脑洞但是这个没有任何证实没有证实它是用了风车数据的就是说我觉得它肯定很可能是用了很多网上视频这些视频是包括了核震数据比如说我有些人网上放一些游戏视频放在YouTube上我理解就是说它如果用了比如说游戏视频生成的那些游戏视频可能是虚幻引擎5做的对但他自己不一定说用虚弯引擎但他没披露他的数据对他没有披露他的数据所以这个其实是一个以讹传讹的一个典型例子吧我觉得充分证明了看第一手资料的重要性对对我觉得就是我当时第一个跳出来说我感觉上这个是不是用Synastic Data然后你可以去看Twitter吧当时搜导出来之后大概一两个小时之内就开始有这种评论所以我觉得这个不是confirm的是不是confirm的OK这个纠正非常好对对我觉得你去看这YouTube videoYouTube video很多也是合成数据对吧游戏数据那是当时合成数据了他们用了一下之后那他们也可以说用合成数据但并不代表他们内部会用细缓引擎生成合成数据不一定是他们做的但是他们可以拿数据过来对那你觉得合成数据的方式去训练大模型你怎么看这种方式呢我觉得这个可能是以后的一个很大的一个趋势我觉得像我们的Searchformer其实就是合成数据就是我们先用传统的方法生成大量的推理步骤然后把这步骤放进Transform里面去训练那么这所有数据都是合成的就是通过已有的引擎去生成一对数据然后去训练那么这个方式其实可以一个是比较有效的能够避开现在数据越来越多越来越难找的一个窘境我想以后应该会有很大的发展那另外一方面合成数据也有自己的一些局限性你想之前围棋这个方向其实都是用了合成数据让围棋软件或者让AI自己跟自己下SelfPlay都是核心数据你这个观点太有意思了你突然提醒了我包括网上我们用的一些训练视频大家觉得是原始数据其实它有可能它就是现在创作者的大量的视频可能也是用核心数据来算的所以我觉得是不是说核心数据跟真实数据它的边界本身也在变得越来越模糊对 我觉得以后边界一定会变得越来越模糊就比如说一个抖音创作者他可能用了虚幻引擎再加上自己最后搞出一个视频来这个视频算不合成数据其实也算但是其实也是真正数据因为是人加工的所以我觉得其实没有特别必要区分之两者吧因为最后可能会越来越模糊所以其实在科研上大家都会用到只要这个东西有帮助我们就都用对它不是一个让大家觉得非常吃惊的一个操作但为什么会问这个问题我们觉得人类的数据是有限的吗现在很多的数据已经被用来去训练OpenAI的大模型包括各个机构的大模型了那接下来我们如果还要沿着大力出奇迹的方式探索像AGI的路数据从哪里来对 一个方法就当然是合成数据了因为合成数据就相当于用算力来换数据你只要有无穷的算力你就有很多很多数据数据会越来越多这是一个办法当然了这个也有问题因为合成出来的数据如果没有human intervention的话它 somehow应该还有一些比较重要的一些信息它其实也不一定能抓住就像OpenAI之前有一篇文章叫Let's verify step by step这篇是做数学推理的他们先生成了大量的一个推理步骤核心数据先让AI来决定那些数据好那些数据不好那AI发现你可以做这些事情但是做完之后呢剩下数据还是要人去过一遍因为剩下数据都是AI觉得很好但其实是错的对那就是相当于如果没有人类的参与的话呢那AI就会在原地转圈子他永远会觉得这个是好的然后去推上但其实这东西是不对的那他就没办法达到更高的那个level所以核心数据有这个问题像围棋的核心数据他的问题就在于他在围棋世界里面可以做到非常厉害他出了世界之后就很难做到这一点最终就是说你可以用核心数据把自己的能力提上去但最终他会遇到一个瓶颈那么这个瓶颈人类能不能帮忙通过摸摸的方式超过这个瓶颈达到下一个level这是一个问题对然后还有一个也是最近的一个新的消息Antheropic发了一个新的大模型我看新闻稿里是说它在推理数学编码还有多余理解跟视觉等20多个测试中性能超过了OpenAI的GPT4我不知道你有没有用过那个模型你感受怎么样它还是挺厉害的但我感觉上因为我是个小说创作者所以我会拿很多段落去测它它其实应该说是感性多于理性就是写文章或者说写小说或者说给小说续一下我觉得它的细节丰富度还是挺好的你觉得它跟GPT4哪个在写小说上表现更好我觉得其实是Antrothic好一点我觉得但是推理上来说我不知道就是我觉得不一定就你没试过推理推理其实试过一些但是它有两方面它特别强一个是长文的理解和分析这个非常非常强我可以把我的小说进去然后它会给你一个章节一个章节做一些总结总结得非常好我觉得这个总结能力其实远远强过GPT4在GPT4有些问题就是一个是说我给他送小说稿的话他会掉code interpreter他会用IG加他的图索引擎那么当然这个IAG的效果就不好了像IAG相当于我就抽一段进来然后用代码的方式生成一个程序去抓里面的数据这个肯定没有原生的用大约模型来做Summarization要好但是Anthotrophic给我的感觉是给他很大一段很长一段话那么这段很长一个故事这故事可能他也没见过但它能够总结得非常好,它的一些心意程度也超过了现在的一些水平,这个是让我觉得非常好的,然后另外就是说虚写什么的也不错,补权什么的都挺好的,但是在推理上来说,给它一些问题它确实也做得挺好,但是你要说比GPT4强我不知道,很难说清楚,我觉得没有非常清楚的界限,它不是一个非常清晰的目标,它是一种人的主观感受,对,所以其实我觉得这个很难讲因为现在网上也看到很多评价说GPT4好像这两天突然之间变强了对就是之前GPT4在偷懒有很多人在抱怨GPT4偷懒说他就可能是干活干得不好这个当然也非常有可能了因为如果这世界上没有竞争者的话我本来可能会决定上线一个比较差的模型以介绍这些代价但是如果有世界上有竞争者的话他们才会出一个更好的模型这个太有意思了所以我们很需要竞争是需要竞争一旦有竞争之后马上就会让大家能真正的感觉到OK我这个模型一定要推出最强的版本其实有很多网上有很多人抱怨了就是GP4刚出来的时候非常惊艳比如说觉得哇好厉害但是越用越差越用越差甚至有人发现他会在周末偷懒周末偷懒就是周末的模型是比周一到周五对对对效果要差的对的就是他可能也学到了人类的一些数据集的一些bias吧如果你发现这个邮件是周末写的可能这个油烟的市场就变差了他会把这个连起来之后呢他可能会自动的有这个bias周末的时候我就起的短一点然后就不会回你的问题或者说他会忽略你的一些情况可能会有这个一个问题但是如果有竞争者的话呢GP4就不得不保证自己质量这个其实是个好事情对接下来我们聊一聊Meta因为Meta其实也开源了嘛我觉得他在市场上还是蛮受关注的你怎么看Meta开源跟他开源的好处我首先先声明我不太想评论Meta的一些东西因为我是Meta的员工所以我不是official的一个news provider言归正传我们在说我们这个故事我觉得开源本身是一个好的我们可以想想大模型的终局吧一种可能是小数巨头垄断市场大家都像个跪拜我不希望这种事情发生另外一种事情发生是人人都有核武器大家形成个威慑平衡我希望后者是成立的如果是这样的话其实我们应该拥抱开源我们应该希望开源做得很厉害开源的话能够让大所有人都用上那么这样的话其实保证一个最好的一个生态吧有竞争才有进步的空间有竞争的话呢大家才会愿意分享愿意把整个世界往前推进这样会比较好对我一直都很好奇开源的商业模式是什么呀general的说一下开源的商业模式我觉得商业模式其实就是转吆喝呗我觉得这是一个因为对Meta来说它不像Google吧对Google来说大摩大用文是一些属于要命的business critical的component因为Google是个service大于模型其实提供了一个口袋版的Google那如果哪天大于模型能力超过Google现在的新搜紧的水平最后的结果就是没有人用Google了这个是对Google不能接受的所以对Google来说它是一个call business所有在训练和推理上的一个优化都不能发表论文应该是这样子的但对于Meta来说Meta的call business它不是这个Meta的call business是人和人之间的连接通过人和人之间的连接来卖广告所以对他来说他当然希望所有人都军事所有人都有自己的大圆模型相互之间的交流用这个方式然后Meta可以作为平台获得一些利益这是我的观点了对他来说开源其实是有利于他的将来的一个发展Meta内部的科研氛围是怎么样的这个可以讲一讲吗我觉得还是比较自由的吧有点像学校的大家表报TenApp可以自由讨论一些问题讨论一些文章最后能够找到一些想法做出来所以其实科研也是一个蛮开放的对还是比较开放的一个方向就是我们还是可以自由地跟其他的院校合作的我记得当时扎克伯格去找了困的时候他说如果我加入你们我的条件就是科研必须以一种开放的方式进行如果你想单独地以一种封闭的方式只在Meta发论文的话我不知道我的工作该如何地进行下去所以他希望所有的东西在学术圈都是公开的对我觉得这个是个很好的一个logic然后我们确实把这个philosophy贯彻到现在了我觉得这是非常好的一个地方就是一开始的创始的时候的一个诺言现在还是能保持对最后一个问题我知道在AI研究业语你自己也是一个科幻作家写了很多很多的科幻小说比如说像优越新火破晓之中血迹梦想济世使命这些非常长篇的科幻故事因为我一直都觉得人工智能科幻包括我们非常多前沿技术有的时候是要靠一点点想象力的所以我才挺好奇你自己的科研从科幻中产生了哪些灵感首先第一点这些小说不一定都是科幻我其实一开始写学院幻的不一定是科幻科幻可能就是从破晓之中开始吧其实之前也有一些科幻学院结合的一些例子但是我觉得其实写小说动因倒不是因为它是科幻小说动因是因为人和人之间的关系比如说我觉得有一些很有趣的场景是应该是它写下来的或者说人生有些经历这个经历让我觉得我应该写下来然后让我能够在时间之后再回味这种经历这种感觉并不是因为科幻小说所以才写科幻小说它的动因不是因为我做research或者说不是因为我做科研动因是因为另外一方面就是感性上的或者说是某种就是人和人之间的交互的这种方式人生经历的方式来动因的所以是自己生活的影子对 是生活的影子这个跟科幻关系倒是不大科幻只是一层皮很多时候是这样子小说最重要的是人和人之间的关系人的角色的塑造这是最重要的你会花多长时间写小说这个要看嘛比如说破小之中大概在2020年年底的时候在职务上连载过那段时间我连连载所以我就不得不每天都要花点时间写一下现在还在连载吗现在已经连载完了破小说已经写完了这部小说之前其实准备了很多时间因为有很多很多的小的interesting idea我要把它写进去说这部份我要写那最后呢你要找到故事把全部连起来所以大概花了五年的时间吧我觉得我大概在刚刚去Meta的时候去Meta的前几年的时候反正就是有些时候有些想法像Interesting那个Spark对吧你可以把它写下来我一般会先写场景场景写完之后再把它的场景连起来变成一部有趣的故事这个过程其实要花一点时间的现在做爸爸了还有时间写科幻小说吗现在其实挺忙的吧而且我们最近研究也挺忙大原模型出来之后我基本上没有什么时间做一些其他的事情但是如果有空的话你还是会继续的吧还是想继续写因为总是会有一些想法那只有一些想法这些想法其实你把记录下来其实很可惜你把记录下来之后你会有一些新的思路新的想法有的时候脑子在换一个思路想一些不同的东西很有意思谁是你的科幻启蒙很难讲吧我也不是特别的明确就是什么样的算一个科幻启蒙吧我觉得就是以前看过漫画呀这些漫画书可能会比较不能说是科幻启蒙应该说是玄幻启蒙或者说是对于一个世界应该怎么构造或者说这个世界什么样东西让我觉得很有意思你喜欢哪些作品中的构造这个其实挺难说的嘛当然三体肯定是一个例子但是三体其实算出来的比较晚了在人家之前应该有很多的暴露你年龄了对 我们都很老了最早比如说漫画像最近鸟山明先生去世了其实我很早以前就特别喜欢看他的青龙珠系列他们这种画得很好的大师它每一帧和每个镜头的切换和那个悬念的塑造都非常好就是直接导致了就是我在写的时候我还是会想很多多线并行的剧情和悬念的塑造和人物的塑造这都会去思考这个可能会潜移默化改变我的一些想法或者一些小说的一些思路吧我觉得我很幸运的是我们在小时候接触的漫画都是顶级的视频真的是那个时候确实是非常好的如果就真的去想整个故事的逻辑的话那其实就会学到很多东西好的非常感谢田博士感谢大家的收听谢谢
